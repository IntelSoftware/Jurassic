{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53fa44a6-07af-4996-8290-0314e0c89cbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test Model\n",
    "\n",
    "Each green square is roughly 260m x 260m or about 2.5 times the length of a soccer of football field\n",
    "\n",
    "<img src=\"assets/TestModel.jpg\" width=\"500\"/>\n",
    "\n",
    "Even though this model was trained on images and data from **New Mexico** it predicts very well in Utah\n",
    "\n",
    "The red circled building is the famous **Dinosaur National Monument** building housing the bones still being excavated from the site.\n",
    "\n",
    "The site was discovered in 1909 by paleontologist Earl Douglass of the Carnegie Museum \n",
    "\n",
    "<img src=\"assets/DNM_Camarasaurus.jpg\" width=\"500\"/>\n",
    "\n",
    "Essentially, the **New Mexico** trained model, predicts **bone likely** at the site of this building in **Utah**!\n",
    "\n",
    "This is based on the texture and color of the depositional environments of sandstones and claystoneâ€™s of the Brushy Basin member of the Morrison Formation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa114f28-c133-4d6b-8270-aabcc6d9dbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Indicate what backbone model was used\n",
    "scratch_model = models.resnet18(pretrained=True)\n",
    "num_ftrs = scratch_model.fc.in_features\n",
    "# modify the backbone to have just three predicted classes\n",
    "classes = 3\n",
    "scratch_model.fc = nn.Linear(num_ftrs, classes)\n",
    "scratch_model.load_state_dict(torch.load('models/resnet18-Gold20220530.pt'))\n",
    "\n",
    "#indicate the path and filename of output image\n",
    "map_save = 'data/MoabDinoTrail_ThreeClassBalanced.jpg'\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "input_size = 224\n",
    "data_dir = \"data/ThreeClassManualRemove0s/\"\n",
    "\n",
    "batch_size = 64\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((input_size,input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((input_size,input_size)),        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"device: {}\".format(device))\n",
    "\n",
    "\n",
    "files = []\n",
    "class_true = []\n",
    "class_pred = []\n",
    "#my_classes = ['Bone', 'NoBone']\n",
    "my_classes = image_datasets['val'].classes\n",
    "\n",
    "green = Image.new('RGBA',(224,224),(0,255,0,60))\n",
    "white = Image.new('RGBA',(224,224),(255,255,255,1))\n",
    "lightGreen = Image.new('RGBA',(224,224),(0,255,0,20))\n",
    "black = Image.new('RGBA',(224,224),(0,0,0,1))\n",
    "\n",
    "\n",
    "def DatasetSizes(dataset_ReadClassChoices):\n",
    "    dataset_sizes = {x: len(dataset_ReadClassChoices[x]) for x in ['train', 'val']}\n",
    "    return dataset_sizes\n",
    "\n",
    "def scoreSingleImage(ImagePath, model, dataset_classes):\n",
    "    from PIL import Image\n",
    "    import torch.nn.functional as F\n",
    "    from torch.autograd import Variable\n",
    "    model.eval()\n",
    "    #model.to(device)\n",
    "    img = Image.open(ImagePath).convert('RGB') \n",
    "    x_test = data_transforms['val'](img)[:3]   #3 channels in case png bobc\n",
    "    x_test.unsqueeze_(0)  # Add batch dimension\n",
    "    x_test2 = Variable(x_test)\n",
    "    output = model(x_test)\n",
    "    class_names = dataset_classes\n",
    "    predArgmax = torch.argmax(output[0]).numpy()\n",
    "    confidence = F.softmax(output, dim=0)\n",
    "    score = []\n",
    "    score.append( class_names[predArgmax] )\n",
    "    score.append( float(confidence[0][predArgmax]) )\n",
    "    return score \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534e42fd-f193-4ac2-8164-f01ee07560ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/ThreeClassManualRemove0s/train/2/FragmentsPresent0008.png'\n",
    "scoreSingleImage(filename, scratch_model, my_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c10de-219f-4fc6-bdcb-64e8dd3b7b07",
   "metadata": {},
   "source": [
    "# Score val folder to print metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dbb0b2-0d63-4961-8b97-efcf2c51c9f0",
   "metadata": {},
   "source": [
    "### Metrics Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009a5f0-ebba-4d99-9c2d-778c986f5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Violence Class\n",
    "def calc_metrics(tp_rowcol, cm):\n",
    "    # this works only for col 0, row 0 for now \n",
    "    #will troubleshoot other columns later\n",
    "    # so its works for Violence but i have not generalized the cal to accomdate other row,col as the tp\n",
    "\n",
    "    tmp = 0\n",
    "    tp_rowcol = -tp_rowcol\n",
    "    tmp = np.roll(cm, tp_rowcol, axis=1)\n",
    "    cm = np.roll(tmp, tp_rowcol, axis=0)  \n",
    "\n",
    "    L = len(cm)\n",
    "    tp = cm[0][0]\n",
    "    fn = sum(cm[0][1:L])\n",
    "    fp = sum(cm, axis = 0)[0] - tp\n",
    "    ftn = sum(cm, axis = 0) - cm[0]\n",
    "    tn = sum(cm[1:L,1:L])\n",
    "    sensitivity_recall =  tp  / (tp + fn + 0.)\n",
    "    specificity =  tn / (tn + fp + 0.)\n",
    "    precision =  tp / (tp + fp + 0.)\n",
    "    accuracy =  (tp+tn+ 0.)/(tp+fp+fn+tn + 0.)\n",
    "    f1 = 2.0*precision*sensitivity_recall/(precision+sensitivity_recall)\n",
    "    return(accuracy, precision, sensitivity_recall, specificity, f1)\n",
    "def print_metrics(accuracy, precision, sensitivity_recall, specificity, f1):\n",
    "    print ('accuracy: ', accuracy)\n",
    "    print ('sensitivity_recall: ',sensitivity_recall)\n",
    "    print ('specificity: ', specificity)\n",
    "    print ('precision: ', precision)\n",
    "    print ('f1: ', f1)\n",
    "def metricsAsDataframe(accuracy, precision, sensitivity_recall, specificity, f1):\n",
    "    data = [{'metric': 'accuracy', 'Value': accuracy, 'Description': '(tp+tn)/(tp+fp+fn+tn)'},\n",
    "             {'metric': 'precision',  'Value': precision, 'Description': 'tp/(tp+fp)' },\n",
    "             {'metric': 'sensitivity_recall',  'Value': sensitivity_recall, 'Description': 'tp  / (tp + fn)'},\n",
    "             {'metric': 'specificity',  'Value': specificity,  'Description': 'tn / (tn + fp)'},\n",
    "            {'metric': 'F1',  'Value': f1,  'Description': '2*precision*recall/(precision+recall)'}]\n",
    "    dfObj = pd.DataFrame(data, columns=['metric', 'Value', 'Description'])\n",
    "    return dfObj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f3878-8c3b-45b1-bdc8-c42351e3ba71",
   "metadata": {},
   "source": [
    "# Score All Validation Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32929b0-eb18-427a-bc42-0f29b48de0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "folders = my_classes\n",
    "files = []\n",
    "class_true = []\n",
    "class_pred = []\n",
    "for fl in folders:\n",
    "    path = data_dir + 'val/' + fl + '/'\n",
    "    for filename in glob.glob(os.path.join(path, '*.png')):\n",
    "        files.append(filename)\n",
    "        try: \n",
    "            pred = scoreSingleImage(filename, scratch_model, my_classes)\n",
    "            class_pred.append(pred[0])\n",
    "            class_true.append(filename.split('/')[-2])\n",
    "        except:\n",
    "            print (\"File not compatible (channels)\", filename)\n",
    "        \n",
    "#[[t, p] for t, p in zip(class_true, class_pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00589185-6b43-470a-b45c-a3fe22bf0b51",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Print Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17855688-77c5-47ba-be05-9da7b99c3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as plt\n",
    "from matplotlib import *\n",
    "import sys\n",
    "from pylab import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = class_true\n",
    "y_pred = class_pred\n",
    "\n",
    "myset = set(y_true)\n",
    "labels = list(myset)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred,  labels=labels)\n",
    "cmd = cm.copy()\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23200297-93c5-42e8-853b-631f652c701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*np.round(cmd/cmd.sum(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b217493-901c-486d-916b-652680e41eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "class_true = np.array(class_true)\n",
    "class_pred = np.array(class_pred)\n",
    "ConfusionMatrixDisplay.from_predictions(class_true, class_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af9b5e-47de-4986-a845-948b76fe9437",
   "metadata": {},
   "source": [
    "If you have any issues or want to contribute, please contact our authors:\n",
    "Intel oneAPI Solution Architect\n",
    "- Chesebrough, Bob [bob.chesebrough (at) intel.com]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b07ea1e-b1af-4e95-8f63-885f4f75d786",
   "metadata": {},
   "source": [
    "## Notices and Disclaimers\n",
    "\n",
    "Intel technologies may require enabled hardware, software or service activation.\n",
    "\n",
    "No product or component can be absolutely secure. \n",
    "\n",
    "Your costs and results may vary. \n",
    "\n",
    "Â© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3ee56-3866-4410-9132-b090247c8cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (AI kit)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
