{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ebc3d07-6666-4685-aa67-1604cb21451b",
   "metadata": {},
   "source": [
    "# Dino Bone Finding PyTorch Model to OpenVINO for High Performant Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fc5437-8c35-44cf-b1ec-7eb3fa919cae",
   "metadata": {},
   "source": [
    "This notebook demonstrates the steps in converting the dinosour bone finding PyTorch model to OpenVINO Intermediate Representation (IR) format.\n",
    "\n",
    "First, the PyTorch model is converted and exported to ONNX. Then, the ONNX model is either feeded directly into OpenVINO Runtime, or the ONNX model is then convert and optimized in the OpenVINO Intermediate Representation (IR) formats. Both ONNX and IR models are executed on OpenVINO Inference Engine to show model inferencing on CPU, iGPU and/or VPU devices interchangably. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06580b-c0c2-446c-b8f9-c505f2da566c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00493b-6b94-4e31-b0af-c19a927aaa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import collections\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from openvino.runtime import Core\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "#sys.path.append(\"../utils\")\n",
    "#import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcb480e-af69-4944-bd35-54d0b46bef9d",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0294ce-cfe1-499d-986b-01391b781d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_NAME = \"models\"\n",
    "BASE_MODEL_NAME = DIRECTORY_NAME + f\"/resnet18-Gold20220530\"\n",
    "\n",
    "# Paths where PyTorch, ONNX and OpenVINO IR models will be stored\n",
    "model_path = Path(BASE_MODEL_NAME).with_suffix(\".pt\")\n",
    "onnx_path = model_path.with_suffix(\".onnx\")\n",
    "ir_path = model_path.with_suffix(\".xml\")\n",
    "\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47aa5db-4ff2-431a-bf04-f701c451ec37",
   "metadata": {},
   "source": [
    "## ONNX Model Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869aedb4-b7a9-4ab4-9b74-3f6479d84430",
   "metadata": {},
   "source": [
    "The output for this cell will show some warnings. These are most likely harmless. Conversion succeeded if the last line of the output says ONNX model exported to resnet18-Gold20220530.onnx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cbb8b5-6b6d-4d8c-9189-8dd1b6112632",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_model = models.resnet18(pretrained=True)\n",
    "num_ftrs = scratch_model.fc.in_features\n",
    "classes = 3\n",
    "scratch_model.fc = nn.Linear(num_ftrs, classes)\n",
    "scratch_model.load_state_dict(torch.load('./models/resnet18-Gold20220530.pt'))\n",
    "#scratch_model.load_state_dict(torch.load('resnet34.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264381fb-d7e7-4feb-95e9-886a044acdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not onnx_path.exists():\n",
    "    dummy_input = torch.randn(1, 3, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "\n",
    "    # For the Fastseg model, setting do_constant_folding to False is required\n",
    "    # for PyTorch>1.5.1\n",
    "    torch.onnx.export(\n",
    "        scratch_model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=False,\n",
    "    )\n",
    "    print(f\"ONNX model exported to {onnx_path}.\")\n",
    "else:\n",
    "    print(f\"ONNX model {onnx_path} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f751eb-994f-48ec-9119-8901004116cd",
   "metadata": {},
   "source": [
    "## Convert ONNX Model to OpenVINO IR Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09405eb9-96d5-430e-a542-b91da979288b",
   "metadata": {},
   "source": [
    "Call the OpenVINO Model Optimizer tool to convert the ONNX model to OpenVINO IR with FP16 precision. The models are saved to the current directory. We add the mean values to the model and scale the output with the standard deviation with --scale_values. With these options, it is not necessary to normalize input data before propagating it through the network.\n",
    "\n",
    "See the Model Optimizer Developer Guide for more information about Model Optimizer.\n",
    "\n",
    "Executing this command may take a while. There may be some errors or warnings in the output. Model Optimization was successful if the last lines of the output include [ SUCCESS ] Generated IR version 11 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d4b85-4955-456f-98ba-14ae55f8a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the command for Model Optimizer\n",
    "mo_command = f\"\"\"mo\n",
    "                 --input_model \"{onnx_path}\"\n",
    "                 --input_shape \"[1,3, {IMAGE_HEIGHT}, {IMAGE_WIDTH}]\"\n",
    "                 --data_type FP16\n",
    "                 --output_dir \"{model_path.parent}\"\n",
    "                 \"\"\"\n",
    "mo_command = \" \".join(mo_command.split())\n",
    "print(\"Model Optimizer command to convert the ONNX model to OpenVINO:\")\n",
    "display(Markdown(f\"`{mo_command}`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5bcbd6-0d57-4d78-8ae3-e64f84f21821",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ir_path.exists():\n",
    "    print(\"Exporting ONNX model to IR... This may take a few minutes.\")\n",
    "    mo_result = %sx $mo_command\n",
    "    print(\"\\n\".join(mo_result))\n",
    "else:\n",
    "    print(f\"IR model {ir_path} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d47cff-669b-49e2-9de3-717b3102a84e",
   "metadata": {},
   "source": [
    "## Run Inference on Single Image with ONNX and IR Format and Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d201669-831e-49b6-a49d-242de5c80d5b",
   "metadata": {},
   "source": [
    "Inference Engine can load ONNX models directly. We first load the ONNX model, do inference and show the results. After that we load the model that was converted to Intermediate Representation (IR) with Model Optimizer and do inference on that model and show the results on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874984f-3f2a-4401-be26-f2ea33b416f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((input_size,input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((input_size,input_size)),        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "ImagePath = Path(\"data/ThreeClassManualRemove0s/val/0/HFNoBone029.png\")\n",
    "img = Image.open(ImagePath)    \n",
    "x_test = data_transforms['val'](img)[:3]   #3 channels in case png bobc\n",
    "x_test.unsqueeze_(0)  # Add batch dimension\n",
    "x_test2 = Variable(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81c8e0-3dc7-4f4a-8fb2-f45dc5af2eb6",
   "metadata": {},
   "source": [
    "### Run Inference on ONNX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b8c1d-9351-45d5-b337-b9e7b13ec85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network to Inference Engine\n",
    "core = Core()\n",
    "model_onnx = core.read_model(model=onnx_path)\n",
    "compiled_model_onnx = core.compile_model(model=model_onnx, device_name=\"CPU\")\n",
    "\n",
    "output_layer_onnx = compiled_model_onnx.output(0)\n",
    "\n",
    "# Run inference on the input image\n",
    "res_onnx = compiled_model_onnx([x_test2])[output_layer_onnx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36add430-2891-46b3-8f39-dd9d331840d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33594ffc-ebc5-4e98-b5ef-88a757778002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax of vector x.\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce61420-2cac-4e07-851a-ea20ba16a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['0', '1', '2']\n",
    "predArgmax = np.argmax(res_onnx)\n",
    "confidence = softmax(res_onnx)\n",
    "score = []\n",
    "score.append(class_names[predArgmax] )\n",
    "score.append(float(confidence[0][predArgmax]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fff4b-a041-4d5e-a303-4e0b8e61c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f71d1f5-1055-440c-9d0d-4149e1a9d17a",
   "metadata": {},
   "source": [
    "### Run Inference on IR Format\n",
    "\n",
    "This will run the model on the CPU using OpenVINO Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb8c02-b64d-45bb-a75f-74bb463ad4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the network in Inference Engine\n",
    "core = Core()\n",
    "model_ir = core.read_model(model=ir_path)\n",
    "compiled_model_ir = core.compile_model(model=model_ir, device_name=\"CPU\")\n",
    "\n",
    "# Get input and output layers\n",
    "output_layer_ir = compiled_model_ir.output(0)\n",
    "\n",
    "# Run inference on the input image\n",
    "res_ir = compiled_model_ir([x_test2])[output_layer_ir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a579f-467c-44ef-a87e-76e7061bfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f370a-ff62-4b0a-892a-48d47601575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predArgmax = np.argmax(res_ir)\n",
    "confidence = softmax(res_ir)\n",
    "score = []\n",
    "score.append(class_names[predArgmax] )\n",
    "score.append(float(confidence[0][predArgmax]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3284a5-fe04-4d6a-848f-9e18de9f65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e184a9f-a207-4258-a6f9-8b55338ea86e",
   "metadata": {},
   "source": [
    "### PyTorch Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55bd45-c029-433b-885d-0e725391a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    result_torch = scratch_model(torch.as_tensor(x_test2).float())\n",
    "\n",
    "predArgmax = torch.argmax(result_torch[0]).numpy()\n",
    "confidence = F.softmax(result_torch, dim=0)\n",
    "score = []\n",
    "score.append( class_names[predArgmax] )\n",
    "score.append( float(confidence[0][predArgmax]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e319add-350c-49d3-a92a-3cf5007e551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c54b40-8306-4a13-a4eb-f711931d9cf4",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f291b-1df7-43ef-943c-a06d98418281",
   "metadata": {},
   "source": [
    "Measure the time it takes to perform inference on 20 images. This gives an indication of performance. For more accurate benchmarking, please use the OpenVINO Benchmark Tool. Note that many optimizations are possible to improve the performance. \n",
    "See examples: https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/104-model-tools/104-model-tools.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be153e91-682a-488a-9a23-f336d43167ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 20\n",
    "\n",
    "input_image = x_test2\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    compiled_model_onnx([x_test2])\n",
    "end = time.perf_counter()\n",
    "time_onnx = end - start\n",
    "print(\n",
    "    f\"ONNX model in Inference Engine/CPU: {time_onnx/num_images:.3f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_onnx:.2f}\"\n",
    ")\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    compiled_model_ir([input_image])\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(\n",
    "    f\"IR model in Inference Engine/CPU: {time_ir/num_images:.3f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_images):\n",
    "        scratch_model(torch.as_tensor(input_image).float())\n",
    "    end = time.perf_counter()\n",
    "    time_torch = end - start\n",
    "print(\n",
    "    f\"PyTorch model on CPU: {time_torch/num_images:.3f} seconds per image, \"\n",
    "    f\"FPS: {num_images/time_torch:.2f}\"\n",
    ")\n",
    "\n",
    "if \"GPU\" in core.available_devices:\n",
    "    compiled_model_onnx_gpu = core.compile_model(model=model_onnx, device_name=\"GPU\")\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_images):\n",
    "        compiled_model_onnx_gpu([input_image])\n",
    "    end = time.perf_counter()\n",
    "    time_onnx_gpu = end - start\n",
    "    print(\n",
    "        f\"ONNX model in Inference Engine/GPU: {time_onnx_gpu/num_images:.3f} \"\n",
    "        f\"seconds per image, FPS: {num_images/time_onnx_gpu:.2f}\"\n",
    "    )\n",
    "\n",
    "    compiled_model_ir_gpu = core.compile_model(model=model_ir, device_name=\"GPU\")\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_images):\n",
    "        compiled_model_ir_gpu([input_image])\n",
    "    end = time.perf_counter()\n",
    "    time_ir_gpu = end - start\n",
    "    print(\n",
    "        f\"IR model in Inference Engine/GPU: {time_ir_gpu/num_images:.3f} \"\n",
    "        f\"seconds per image, FPS: {num_images/time_ir_gpu:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d412213-2900-4dcd-a9d6-e0d11b674b05",
   "metadata": {},
   "source": [
    "## Results of Inference on Image Folder and Map Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5918a5-1d9d-409d-9593-6653830ec493",
   "metadata": {},
   "source": [
    "Best map to score\n",
    "Green square roughly 265 m x 265 m -  about 2.5 football or soccer fields long\n",
    "The are in Green is a significantly smaller search are than the entire map\n",
    "\n",
    "Run Inference on the whole folder of 224 maps and merge the final results into a big map.\n",
    "Compare the results with inference on ONNX, IR files and Pytorch format with CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ab92f-f2b6-4861-885f-152848864000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(ImagePath):\n",
    "    img = Image.open(ImagePath)    \n",
    "    x_test = data_transforms['val'](img)[:3]   #3 channels in case png bobc\n",
    "    x_test.unsqueeze_(0)  # Add batch dimension\n",
    "    x_test2 = Variable(x_test)\n",
    "    return x_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52018e51-5ec2-4bee-ba7e-fc40f0833c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreSingleImage_onnx(ImagePath, compiled_model_onnx, output_layer_onnx, dataset_classes):\n",
    "    \"\"\"Run inference on single image with ONNX format.\n",
    "    \"\"\"\n",
    "    x_test2 = image_preprocessing(ImagePath)\n",
    "    \n",
    "    start_infer = time.perf_counter()\n",
    "    # Run inference on the input image\n",
    "    res_onnx = compiled_model_onnx([x_test2])[output_layer_onnx]\n",
    "    stop_infer = time.perf_counter()\n",
    "    time_infer = stop_infer - start_infer\n",
    "    \n",
    "    class_names = dataset_classes\n",
    "    predArgmax = np.argmax(res_onnx)\n",
    "    confidence = softmax(res_onnx)\n",
    "    score = []\n",
    "    score.append(class_names[predArgmax] )\n",
    "    score.append(float(confidence[0][predArgmax]) )\n",
    "    return score, time_infer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aed235-853e-46f8-a317-d042a3db695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreSingleImage_ir(ImagePath, compiled_model_ir, output_layer_ir, dataset_classes):\n",
    "    \"\"\"Run inference on single image with IR format.\n",
    "    \"\"\"\n",
    "    x_test2 = image_preprocessing(ImagePath)\n",
    "    \n",
    "    start_infer = time.perf_counter()\n",
    "    # Run inference on the input image\n",
    "    res_ir = compiled_model_ir([x_test2])[output_layer_ir]\n",
    "    stop_infer = time.perf_counter()\n",
    "    time_infer = stop_infer - start_infer\n",
    "    \n",
    "    predArgmax = np.argmax(res_ir)\n",
    "    confidence = softmax(res_ir)\n",
    "    score = []\n",
    "    score.append(class_names[predArgmax] )\n",
    "    score.append(float(confidence[0][predArgmax]) )\n",
    "    return score, time_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98827cdd-e8d8-4833-aea4-ec0ceb24351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreSingleImage_torch(ImagePath, model, dataset_classes):\n",
    "    model.eval()\n",
    "    #model.to(device)\n",
    "    x_test2 = image_preprocessing(ImagePath)\n",
    "    \n",
    "    start_infer = time.perf_counter()\n",
    "    output = model(x_test2)\n",
    "    stop_infer = time.perf_counter()\n",
    "    time_infer = stop_infer - start_infer\n",
    "\n",
    "    class_names = dataset_classes\n",
    "    predArgmax = torch.argmax(output[0]).numpy()\n",
    "    confidence = F.softmax(output, dim=0)\n",
    "    score = []\n",
    "    score.append( class_names[predArgmax] )\n",
    "    score.append( float(confidence[0][predArgmax]) )\n",
    "    return score, time_infer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543895a4-c812-4fcb-9f80-3fef4408982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_to_map(pred, filenameMap):\n",
    "    \"\"\"Color each 224 * 224 map with inference results corresponded colors.\n",
    "       Green for 'Bones highly likely', lightGreen for 'Bones possible', and white for 'No Bones'.\n",
    "    \"\"\"\n",
    "    if pred[0] == '2':\n",
    "        Image.alpha_composite(img.convert(\"RGBA\"), green).save(filenameMap)\n",
    "    elif pred[0] == '1': \n",
    "        Image.alpha_composite(img.convert(\"RGBA\"), lightGreen).save(filenameMap)\n",
    "    else:\n",
    "        Image.alpha_composite(img.convert(\"RGBA\"), black).save(filenameMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb51bbe-3ca8-4961-b123-cdd73f7d0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_merge_map(merge_path):\n",
    "    \"\"\" Merge the 224 * 224 map into a large map for the final bone finding results.\n",
    "    \"\"\"\n",
    "    map_save = merge_path\n",
    "    xblock = 17\n",
    "    yblock = 15\n",
    "    dst = Image.new('RGB', ((xblock - 1)*224, (yblock - 1)*224))\n",
    "    for x in range(xblock):\n",
    "        for y in range(yblock):\n",
    "            path = 'data/DinosaurNationalMonument/20220514/224Map/DNM_x{:02d}y{:02d}.png'.format(x,y)\n",
    "            img = Image.open(path)\n",
    "            dst.paste(img, (x*224, y*224))\n",
    "            img.close()\n",
    "    dst.save(map_save)\n",
    "    print(map_save)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a47fa-1fb3-4af8-8f1a-65c218388508",
   "metadata": {},
   "outputs": [],
   "source": [
    "green = Image.new('RGBA',(224,224),(0,255,0,60))\n",
    "white = Image.new('RGBA',(224,224),(255,255,255,1))\n",
    "lightGreen = Image.new('RGBA',(224,224),(0,255,0,20))\n",
    "black = Image.new('RGBA',(224,224),(0,0,0,1))\n",
    "\n",
    "my_classes = ['0', '1', '2']\n",
    "\n",
    "lookup = {'0': 'No Bones',\n",
    "          '1': 'Bones possible',\n",
    "          '2': 'Bones highly likely'\n",
    "         }\n",
    "#class_true = []\n",
    "#class_pred = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a0114-3f25-4148-8ffd-9ed958a6d6e6",
   "metadata": {},
   "source": [
    "# Running inference on the input images with PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec0caa-aaf0-495c-a419-edff1f9b1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_model = models.resnet18(pretrained=True)\n",
    "num_ftrs = scratch_model.fc.in_features\n",
    "classes = 3\n",
    "scratch_model.fc = nn.Linear(num_ftrs, classes)\n",
    "scratch_model.load_state_dict(torch.load('./models/resnet18-Gold20220530.pt'))\n",
    "score_torch = []\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "infer_time_sum = 0\n",
    "xblock = 17\n",
    "yblock = 15\n",
    "for x in range(xblock):\n",
    "    for y in range(yblock):\n",
    "        filename = 'data/DinosaurNationalMonument/20220514/224/DNM_x{:02d}y{:02d}.jpg'.format(x, y)\n",
    "        filenameMap = 'data/DinosaurNationalMonument/20220514/224Map/DNM_x{:02d}y{:02d}.png'.format(x, y)\n",
    "        img = Image.open(filename)\n",
    "        try: \n",
    "            pred, time_infer = scoreSingleImage_torch(filename, scratch_model, my_classes)\n",
    "            infer_time_sum += time_infer\n",
    "            score_torch.append(pred)\n",
    "            #print(filename.split('/')[-1], lookup[pred[0]])\n",
    "            result_to_map(pred, filenameMap)\n",
    "            # Currently, the filename in '224' data folder doesn't imply the truth label\n",
    "            # Therefore, commenting the following commands for future confusion matrix and accuracy check\n",
    "            #class_pred.append(pred[0])\n",
    "            #class_true.append(filename.split('/')[-2])\n",
    "        except:\n",
    "            print (\"Problem\", x, y, filename)\n",
    "\n",
    "print(\"Scoring time elapsed for Pytorch: \", time.perf_counter() - start_time) \n",
    "print(\"Inferencing time elapsed for Pytorch: \", infer_time_sum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd0cea-1bf7-41e1-96fb-b78e4731fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc190bb-a619-4727-9cee-17e75deab7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_path = 'data/DNM_ThreeClassBalanced_openvino.jpg'\n",
    "save_merge_map(merge_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa9c0a8-c327-4b39-b4c2-de4b20a548e1",
   "metadata": {},
   "source": [
    "# Running inference on the input images with OpenVINO \n",
    "This will run the inference on OpenVINO. If GPU is available, the code will also automatically offload the work to GPUs. \n",
    "\n",
    "Note: A larger sample size is ideal for benchmarking the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f843f7-e115-4085-b00b-ee6cca8d6387",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model_ir_cpu_gpu = core.compile_model(model=model_ir, device_name=\"AUTO\")\n",
    "output_layer_ir = compiled_model_ir_cpu_gpu.output(0)\n",
    "score_ir = []\n",
    "start_time = time.perf_counter()\n",
    "infer_time_sum = 0\n",
    "xblock = 17\n",
    "yblock = 15\n",
    "for x in range(xblock):\n",
    "    for y in range(yblock):\n",
    "        filename = 'data/DinosaurNationalMonument/20220514/224/DNM_x{:02d}y{:02d}.jpg'.format(x, y)\n",
    "        filenameMap = 'data/DinosaurNationalMonument/20220514/224Map/DNM_x{:02d}y{:02d}.png'.format(x, y)\n",
    "        img = Image.open(filename)\n",
    "        try: \n",
    "            pred, time_infer = scoreSingleImage_ir(filename, compiled_model_ir_cpu_gpu, output_layer_ir, my_classes)\n",
    "            infer_time_sum += time_infer\n",
    "            score_ir.append(pred)\n",
    "            #print(filename.split('/')[-1], lookup[pred[0]])\n",
    "            result_to_map(pred, filenameMap)\n",
    "            # Currently, the filename in '224' data folder doesn't imply the truth label\n",
    "            # Therefore, commenting the following commands for future confusion matrix and accuracy check\n",
    "            #class_pred.append(pred[0])\n",
    "            #class_true.append(filename.split('/')[-2])\n",
    "        except:\n",
    "            print (\"Problem\", x, y, filename)\n",
    "print(\"Scoring time elapsed for IR: \", time.perf_counter() - start_time) \n",
    "print(\"Inferencing time elapsed for Pytorch: \", infer_time_sum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a3574-3bdf-42d4-9eab-418a46ee9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_path = 'data/DNM_ThreeClassBalanced_ir.jpg'\n",
    "save_merge_map(merge_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c549fb0-58e4-4c83-8b90-c121ea2f526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887acb25-e141-4a92-aef7-1ceac35f5a16",
   "metadata": {},
   "source": [
    "# Compare the results/accuracy between PyTorch, ONNX, and OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e05236-f0db-4398-847f-f7ec4e8af106",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for k in range(len(score_torch)):\n",
    "    if score_torch[k][0] != score_onnx[k][0]:\n",
    "        counter += 1\n",
    "        print('comparing between onnx and pytorch')\n",
    "        print(k)\n",
    "        print(score_torch[k][0])\n",
    "        print(score_onnx[k][0])\n",
    "    if score_torch[k][0] != score_ir[k][0]:\n",
    "        counter += 1\n",
    "        print('comparing between pytorch and ir')\n",
    "        print(k)\n",
    "        print(score_torch[k][0])\n",
    "        print(score_ir[k][0])\n",
    "print('counter is', counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dfaeb3-7d74-411c-9ac1-63967f385b19",
   "metadata": {},
   "source": [
    "## To learn more about OpenVINO. Try the demo link below.\n",
    "* https://github.com/openvinotoolkit/openvino_notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65def6ee-32a7-40b0-8c06-53cf5bce17c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
