{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ebc3d07-6666-4685-aa67-1604cb21451b",
   "metadata": {},
   "source": [
    "# Dino Bone Finding PyTorch Model to OpenVINO for High Performant Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fc5437-8c35-44cf-b1ec-7eb3fa919cae",
   "metadata": {},
   "source": [
    "This notebook demonstrates the steps in converting the dinosour bone finding PyTorch model to OpenVINO Intermediate Representation (IR) format.\n",
    "\n",
    "First, the PyTorch model is converted and exported to ONNX. Then, the ONNX model is either feeded directly into OpenVINO Runtime, or the ONNX model is then convert and optimized in the OpenVINO Intermediate Representation (IR) formats. Both ONNX and IR models are executed on OpenVINO Inference Engine to show model inferencing on CPU, iGPU and/or VPU devices interchangably. \n",
    "\n",
    "Authors: Zhuo Wu, PhD (zhuo.wu@intel.com)  Raymond Lo, PhD (OpenVINO Edge AI Software Evangelist - Intel) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06580b-c0c2-446c-b8f9-c505f2da566c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00493b-6b94-4e31-b0af-c19a927aaa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from openvino.runtime import AsyncInferQueue, Core, InferRequest, Layout, Type\n",
    "import openvino.runtime as ov\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcb480e-af69-4944-bd35-54d0b46bef9d",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90fef3d-93b9-4105-af36-19549aabdcc2",
   "metadata": {},
   "source": [
    "Set the name for the model. Define the path for the trained dinosaur bone finding model with PyTorch format, and the path for the converted onnx and OpenVINO IR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0294ce-cfe1-499d-986b-01391b781d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_NAME = \"models\"\n",
    "BASE_MODEL_NAME = DIRECTORY_NAME + f\"/resnet18-Gold20220530\"\n",
    "BASE_MODEL_NAME = DIRECTORY_NAME + f\"/bc_resnet18_simple_NOIPEX_6Epochs_StateDict_gold\"\n",
    "#BASE_MODEL_NAME = DIRECTORY_NAME + f\"/bc_resnet18_simple_NOIPEX_30Epochs_gold\"\n",
    "\n",
    "# Paths where PyTorch, ONNX and OpenVINO IR models will be stored\n",
    "model_path = Path(BASE_MODEL_NAME).with_suffix(\".pt\")\n",
    "onnx_path = model_path.with_suffix(\".onnx\")\n",
    "ir_path = model_path.with_suffix(\".xml\")\n",
    "\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47aa5db-4ff2-431a-bf04-f701c451ec37",
   "metadata": {},
   "source": [
    "## ONNX Model Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869aedb4-b7a9-4ab4-9b74-3f6479d84430",
   "metadata": {},
   "source": [
    "The output for this cell will show some warnings. These are most likely harmless. Conversion succeeded if the last line of the output says ONNX model exported to resnet18-Gold20220530.onnx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cbb8b5-6b6d-4d8c-9189-8dd1b6112632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch_model = models.resnet18(pretrained=True)\n",
    "# num_ftrs = scratch_model.fc.in_features\n",
    "# classes = 3\n",
    "# scratch_model.fc = nn.Linear(num_ftrs, classes)\n",
    "# #scratch_model.load_state_dict(torch.load('./models/resnet18-Gold20220530.pt'))\n",
    "# scratch_model.load_state_dict(torch.load('./models/bc_resnet18_simple_NOIPEX_30Epochs.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59fad7-7716-4614-923e-1f17f85c8a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_model = models.resnet18(pretrained=True)\n",
    "num_ftrs = scratch_model.fc.in_features\n",
    "classes = 3\n",
    "scratch_model.fc = nn.Linear(num_ftrs, classes)\n",
    "#scratch_model.load_state_dict(torch.load('./models/resnet18-Gold20220530.pt'))\n",
    "scratch_model.load_state_dict(torch.load(f'./{BASE_MODEL_NAME}.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264381fb-d7e7-4feb-95e9-886a044acdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not onnx_path.exists():\n",
    "    dummy_input = torch.randn(1, 3, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "\n",
    "    # For the Fastseg model, setting do_constant_folding to False is required\n",
    "    # for PyTorch>1.5.1\n",
    "    torch.onnx.export(\n",
    "        scratch_model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=False,\n",
    "    )\n",
    "    print(f\"ONNX model exported to {onnx_path}.\")\n",
    "else:\n",
    "    print(f\"ONNX model {onnx_path} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f751eb-994f-48ec-9119-8901004116cd",
   "metadata": {},
   "source": [
    "## Convert ONNX Model to OpenVINO IR Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09405eb9-96d5-430e-a542-b91da979288b",
   "metadata": {},
   "source": [
    "Call the OpenVINO Model Optimizer tool to convert the ONNX model to OpenVINO IR with FP16 precision. The models are saved to the current directory. We add the mean values to the model and scale the output with the standard deviation with --scale_values. With these options, it is not necessary to normalize input data before propagating it through the network.\n",
    "\n",
    "See the [Model Optimizer Developer Guide](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) for more information about Model Optimizer.\n",
    "\n",
    "Executing this command may take a while. There may be some errors or warnings in the output. Model Optimization was successful if the last lines of the output include [ SUCCESS ] Generated IR version 11 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d4b85-4955-456f-98ba-14ae55f8a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the command for Model Optimizer\n",
    "mo_command = f\"\"\"mo\n",
    "                 --input_model \"{onnx_path}\"\n",
    "                 --input_shape \"[1,3, {IMAGE_HEIGHT}, {IMAGE_WIDTH}]\"\n",
    "                 --data_type FP16\n",
    "                 --output_dir \"{model_path.parent}\"\n",
    "                 \"\"\"\n",
    "mo_command = \" \".join(mo_command.split())\n",
    "print(\"Model Optimizer command to convert the ONNX model to OpenVINO:\")\n",
    "display(Markdown(f\"`{mo_command}`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5bcbd6-0d57-4d78-8ae3-e64f84f21821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not ir_path.exists():\n",
    "    print(\"Exporting ONNX model to IR... This may take a few minutes.\")\n",
    "    mo_result = %sx $mo_command\n",
    "    print(\"\\n\".join(mo_result))\n",
    "    if 'bad interpreter' in mo_result[0]:\n",
    "        print(\"something went wrong\")\n",
    "        print(\"run the mo_command string from the command line after activating openvinopytorch\")\n",
    "        print(\"copy this line to command line:\")\n",
    "        print(mo_command)\n",
    "else:\n",
    "    print(f\"IR model {ir_path} already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d47cff-669b-49e2-9de3-717b3102a84e",
   "metadata": {},
   "source": [
    "## Run Inference on Single Image with ONNX and IR Format and Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d201669-831e-49b6-a49d-242de5c80d5b",
   "metadata": {},
   "source": [
    "Inference Engine can load ONNX models directly. We first load the ONNX model, do inference and show the results. After that we load the model that was converted to Intermediate Representation (IR) with Model Optimizer and do inference on that model and show the results on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874984f-3f2a-4401-be26-f2ea33b416f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((input_size,input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((input_size,input_size)),        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "ImagePath = Path(\"assets/HFNoBone029.png\")\n",
    "img = Image.open(ImagePath)    \n",
    "x_test = data_transforms['val'](img)[:3]   #3 channels in case png bobc\n",
    "x_test.unsqueeze_(0)  # Add batch dimension\n",
    "x_test2 = Variable(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81c8e0-3dc7-4f4a-8fb2-f45dc5af2eb6",
   "metadata": {},
   "source": [
    "### Run Inference on ONNX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b8c1d-9351-45d5-b337-b9e7b13ec85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network to Inference Engine\n",
    "core = Core()\n",
    "model_onnx = core.read_model(model=onnx_path)\n",
    "compiled_model_onnx = core.compile_model(model=model_onnx, device_name=\"CPU\")\n",
    "\n",
    "output_layer_onnx = compiled_model_onnx.output(0)\n",
    "\n",
    "# Run inference on the input image\n",
    "res_onnx = compiled_model_onnx([x_test2])[output_layer_onnx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36add430-2891-46b3-8f39-dd9d331840d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the inference results using the onnx model format\n",
    "print(\"inference result using onnx format for the 3 classes are \", res_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33594ffc-ebc5-4e98-b5ef-88a757778002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax of vector x.\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce61420-2cac-4e07-851a-ea20ba16a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['0', '1', '2']\n",
    "predArgmax = np.argmax(res_onnx)\n",
    "confidence = softmax(res_onnx)\n",
    "score = []\n",
    "score.append(class_names[predArgmax] )\n",
    "score.append(float(confidence[0][predArgmax]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fff4b-a041-4d5e-a303-4e0b8e61c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the final score, including the class name for the input map image, and its confidence level\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f71d1f5-1055-440c-9d0d-4149e1a9d17a",
   "metadata": {},
   "source": [
    "### Run Inference on IR Format\n",
    "\n",
    "This will run the model on the CPU using OpenVINO Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb8c02-b64d-45bb-a75f-74bb463ad4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the network in Inference Engine\n",
    "core = Core()\n",
    "model_ir = core.read_model(model=ir_path)\n",
    "compiled_model_ir = core.compile_model(model=model_ir, device_name=\"CPU\")\n",
    "\n",
    "# Get input and output layers\n",
    "output_layer_ir = compiled_model_ir.output(0)\n",
    "\n",
    "# Run inference on the input image\n",
    "res_ir = compiled_model_ir([x_test2])[output_layer_ir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346dcf40-d871-4f3b-95fa-928e9d2f41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input and output nodes\n",
    "input_layer = compiled_model_ir.input\n",
    "output_layer = compiled_model_ir.output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a579f-467c-44ef-a87e-76e7061bfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the inference results using the IR model format\n",
    "print(\"inference result using IR format for the 3 classes are \", res_ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f370a-ff62-4b0a-892a-48d47601575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predArgmax = np.argmax(res_ir)\n",
    "confidence = softmax(res_ir)\n",
    "score = []\n",
    "score.append(class_names[predArgmax] )\n",
    "score.append(float(confidence[0][predArgmax]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3284a5-fe04-4d6a-848f-9e18de9f65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the final score, including the class name for the input map image, and its confidence level\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e184a9f-a207-4258-a6f9-8b55338ea86e",
   "metadata": {},
   "source": [
    "### PyTorch Score Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e5ef3-277d-4096-8d4b-5ca3e8ef0e64",
   "metadata": {},
   "source": [
    "Compare the inference results using the PyTorch model format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55bd45-c029-433b-885d-0e725391a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    result_torch = scratch_model(torch.as_tensor(x_test2).float())\n",
    "\n",
    "predArgmax = torch.argmax(result_torch[0]).numpy()\n",
    "confidence = F.softmax(result_torch, dim=0)\n",
    "score = []\n",
    "score.append( class_names[predArgmax] )\n",
    "score.append( float(confidence[0][predArgmax]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e319add-350c-49d3-a92a-3cf5007e551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the final score, including the class name for the input map image, and its confidence level\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c54b40-8306-4a13-a4eb-f711931d9cf4",
   "metadata": {},
   "source": [
    "## Performance Comparison on Single Image Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f291b-1df7-43ef-943c-a06d98418281",
   "metadata": {},
   "source": [
    "Measure the time it takes to perform inference on 20 images. This gives an indication of performance. For more accurate benchmarking, please use the OpenVINO Benchmark Tool. Note that many optimizations are possible to improve the performance. \n",
    "See examples: https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/104-model-tools/104-model-tools.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be153e91-682a-488a-9a23-f336d43167ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 400\n",
    "\n",
    "input_image = x_test2\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_images):\n",
    "        scratch_model(torch.as_tensor(input_image).float())\n",
    "    end = time.perf_counter()\n",
    "    time_torch = end - start\n",
    "print(\n",
    "    f\"PyTorch model on CPU: {time_torch/num_images:.3f} seconds per image, \"\n",
    "    f\"FPS: {num_images/time_torch:.2f}\"\n",
    ")\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    compiled_model_onnx([x_test2])\n",
    "end = time.perf_counter()\n",
    "time_onnx = end - start\n",
    "print(\n",
    "    f\"ONNX model in Inference Engine/CPU: {time_onnx/num_images:.3f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_onnx:.2f}\"\n",
    ")\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    compiled_model_ir([input_image])\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(\n",
    "    f\"IR model in Inference Engine/CPU: {time_ir/num_images:.3f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")\n",
    "\n",
    "if \"GPU\" in core.available_devices:\n",
    "    compiled_model_onnx_gpu = core.compile_model(model=model_onnx, device_name=\"GPU\")\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_images):\n",
    "        compiled_model_onnx_gpu([input_image])\n",
    "    end = time.perf_counter()\n",
    "    time_onnx_gpu = end - start\n",
    "    print(\n",
    "        f\"ONNX model in Inference Engine/GPU: {time_onnx_gpu/num_images:.3f} \"\n",
    "        f\"seconds per image, FPS: {num_images/time_onnx_gpu:.2f}\"\n",
    "    )\n",
    "\n",
    "    compiled_model_ir_gpu = core.compile_model(model=model_ir, device_name=\"GPU\")\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_images):\n",
    "        compiled_model_ir_gpu([input_image])\n",
    "    end = time.perf_counter()\n",
    "    time_ir_gpu = end - start\n",
    "    print(\n",
    "        f\"IR model in Inference Engine/GPU: {time_ir_gpu/num_images:.3f} \"\n",
    "        f\"seconds per image, FPS: {num_images/time_ir_gpu:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d3b2b-5df2-412c-a6e2-32a6b99be8bd",
   "metadata": {},
   "source": [
    "## Run Inference with Images Tiling from Big Map with Single Image Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1de97-91ef-4eeb-bd0a-5898d7d49317",
   "metadata": {},
   "source": [
    "Best map to score Green square roughly 265 m x 265 m - about 2.5 football or soccer fields long The are in Green is a significantly smaller search are than the entire map. \\\n",
    "Run Inference on the whole folder of 224 maps and merge the final results into a big map. This time, image batches are first created, and then the inference will be performed on batches. Results are compared between PyTorch and OpenVINO IR formats on image batches. \\\n",
    "Define pre-processing functions for images tiling from a big map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ab92f-f2b6-4861-885f-152848864000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_preprocessing(ImagePath):\n",
    "    \"\"\"Preprocessing for the input aerial 224*224 map image.\n",
    "    \"\"\"\n",
    "    img = Image.open(ImagePath)    \n",
    "    x_test = data_transforms['val'](img)[:3]   #3 channels in case png bobc\n",
    "    x_test.unsqueeze_(0)  # Add batch dimension\n",
    "    x_test2 = Variable(x_test)\n",
    "    return x_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696020f9-c640-4f53-952b-d1d8bf261024",
   "metadata": {},
   "outputs": [],
   "source": [
    "green = Image.new('RGBA',(224,224),(0,255,0,60))\n",
    "white = Image.new('RGBA',(224,224),(255,255,255,1))\n",
    "lightGreen = Image.new('RGBA',(224,224),(0,255,0,20))\n",
    "black = Image.new('RGBA',(224,224),(0,0,0,1))\n",
    "\n",
    "my_classes = ['0', '1', '2']\n",
    "\n",
    "lookup = {'0': 'No Bones',\n",
    "          '1': 'Bones possible',\n",
    "          '2': 'Bones highly likely'\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa251972-94f8-406e-95e8-4e3334c0f431",
   "metadata": {},
   "source": [
    "### Running single image inference on the input images with PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98827cdd-e8d8-4833-aea4-ec0ceb24351b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scoreSingleImage_torch(ImagePath, model, dataset_classes):\n",
    "    \"\"\"Run inference on single image with PyTorch format.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x_test2 = image_preprocessing(ImagePath)\n",
    "    \n",
    "    start_infer = time.perf_counter()\n",
    "    output = model(x_test2)\n",
    "    stop_infer = time.perf_counter()\n",
    "    time_infer = stop_infer - start_infer\n",
    "\n",
    "    class_names = dataset_classes\n",
    "    predArgmax = torch.argmax(output[0]).numpy()\n",
    "    confidence = F.softmax(output, dim=0)\n",
    "    score = []\n",
    "    score.append( class_names[predArgmax] )\n",
    "    score.append( float(confidence[0][predArgmax]) )\n",
    "    return score, time_infer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ff3d4-eb44-4157-8d7c-8bcc13bf8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_to_map(pred, filenameMap):\n",
    "    \"\"\"Color each 224 * 224 map with inference results corresponded colors.\n",
    "       Green for 'Bones highly likely', lightGreen for 'Bones possible', and white for 'No Bones'.\n",
    "    \"\"\"\n",
    "    if pred == '2':\n",
    "        Image.alpha_composite(img.convert(\"RGBA\"), green).save(filenameMap)\n",
    "    elif pred == '1': \n",
    "        Image.alpha_composite(img.convert(\"RGBA\"), lightGreen).save(filenameMap)\n",
    "    else:\n",
    "        Image.alpha_composite(img.convert(\"RGBA\"), black).save(filenameMap)\n",
    "        \n",
    "def save_merge_map(merge_path):\n",
    "    \"\"\" Merge the 224 * 224 map into a large map for the final bone finding results.\n",
    "    \"\"\"\n",
    "    map_save = merge_path\n",
    "    xblock = 17\n",
    "    yblock = 15\n",
    "    dst = Image.new('RGB', ((xblock - 1)*224, (yblock - 1)*224))\n",
    "    for x in range(xblock):\n",
    "        for y in range(yblock):\n",
    "            path = 'assets/224Map_auto/DNM_x{:02d}y{:02d}.png'.format(x,y)\n",
    "            img = Image.open(path)\n",
    "            dst.paste(img, (x*224, y*224))\n",
    "            img.close()\n",
    "    dst.save(map_save)\n",
    "    print(map_save)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229eb22-103c-45ba-a53b-53bb31621d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "scratch_model = models.resnet18(pretrained=True)\n",
    "num_ftrs = scratch_model.fc.in_features\n",
    "classes = 3\n",
    "scratch_model.fc = nn.Linear(num_ftrs, classes)\n",
    "scratch_model.load_state_dict(torch.load(model_path))\n",
    "score_torch = []\n",
    "mapPath = 'assets/224Map'\n",
    "if not os.path.exists(mapPath):\n",
    "    os.makedirs(mapPath)\n",
    "start_time = time.perf_counter()\n",
    "infer_time_sum = 0\n",
    "xblock = 17\n",
    "yblock = 15\n",
    "for x in range(xblock):\n",
    "    for y in range(yblock):\n",
    "        filename = 'assets/224/DNM_x{:02d}y{:02d}.jpg'.format(x, y)\n",
    "        filenameMap = 'assets/224Map/DNM_x{:02d}y{:02d}.png'.format(x, y)\n",
    "        img = Image.open(filename)\n",
    "        try: \n",
    "            pred, time_infer = scoreSingleImage_torch(filename, scratch_model, my_classes)\n",
    "            infer_time_sum += time_infer\n",
    "            score_torch.append(pred)\n",
    "            result_to_map(pred, filenameMap)\n",
    "        except:\n",
    "            print (\"Problem\", x, y, filename)\n",
    "\n",
    "print(\"Scoring time elapsed for Pytorch: \", time.perf_counter() - start_time) \n",
    "print(\"Inferencing time elapsed for Pytorch: \", infer_time_sum) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18616690-80c1-41c2-ab9e-b50a33cd1fff",
   "metadata": {},
   "source": [
    "### Running single image inference on the input images with OpenVINO IR format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb19286-9c28-4691-ae5b-e92dfafd5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreSingleImage_ir(ImagePath, compiled_model_ir, output_layer_ir, dataset_classes):\n",
    "    \"\"\"Run inference on single image with IR format.\n",
    "    \"\"\"\n",
    "    x_test2 = image_preprocessing(ImagePath)\n",
    "    \n",
    "    start_infer = time.perf_counter()\n",
    "    # Run inference on the input image\n",
    "    res_ir = compiled_model_ir([x_test2])[output_layer_ir]    \n",
    "    stop_infer = time.perf_counter()\n",
    "    time_infer = stop_infer - start_infer\n",
    "    \n",
    "    predArgmax = np.argmax(res_ir)\n",
    "    confidence = softmax(res_ir)\n",
    "    score = []\n",
    "    score.append(class_names[predArgmax] )\n",
    "    score.append(float(confidence[0][predArgmax]) )\n",
    "    return score, time_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007cc67-e619-432e-8c05-f079882f3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ir = core.read_model(model=ir_path)\n",
    "config = {\"PERFORMANCE_HINT\": \"THROUGHPUT\",\n",
    "          \"ALLOW_AUTO_BATCHING\": \"Yes\"}\n",
    "compiled_model_ir_cpu_gpu = core.compile_model(model_ir, \"GPU\")\n",
    "output_layer_ir = compiled_model_ir_cpu_gpu.output(0)\n",
    "score_ir = []\n",
    "start_time = time.perf_counter()\n",
    "infer_time_sum = 0\n",
    "xblock = 17\n",
    "yblock = 15\n",
    "mapPath = 'assets/224Map_auto'\n",
    "if not os.path.exists(mapPath):\n",
    "    os.makedirs(mapPath)\n",
    "for x in range(xblock):\n",
    "    for y in range(yblock):\n",
    "        filename = 'assets/224/DNM_x{:02d}y{:02d}.jpg'.format(x, y)\n",
    "        filenameMap = 'assets/224Map_auto/DNM_x{:02d}y{:02d}.png'.format(x, y)\n",
    "        img = Image.open(filename)\n",
    "        try: \n",
    "            pred, time_infer = scoreSingleImage_ir(filename, compiled_model_ir_cpu_gpu, output_layer_ir, my_classes)\n",
    "            infer_time_sum += time_infer\n",
    "            score_ir.append(pred)\n",
    "            # print out the predicted class names for the input images\n",
    "            # print(filename.split('/')[-1], lookup[pred[0]])\n",
    "            result_to_map(pred, filenameMap)\n",
    "        except:\n",
    "            print (\"Problem\", x, y, filename)\n",
    "print(\"Scoring time elapsed for IR: \", time.perf_counter() - start_time) \n",
    "print(\"Inferencing time elapsed for OpenVINO IR: \", infer_time_sum) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552c94b-2c9e-4e31-b63f-e31d14866674",
   "metadata": {},
   "source": [
    "### Compare the results/accuracy between PyTorch and OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda57b8-c6ba-4e02-bd6b-a38797a4020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for k in range(len(score_ir)):\n",
    "    if score_torch[k][0] != score_ir[k][0]:\n",
    "    #if int(score_auto[k]) != int(score_ir[k][0]):\n",
    "        counter += 1\n",
    "        #print('comparing between pytorch and ir')\n",
    "        #print(k)\n",
    "        #print(score_batch[k])\n",
    "        #print(score_ir[k][0])\n",
    "print('counter is', counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d412213-2900-4dcd-a9d6-e0d11b674b05",
   "metadata": {},
   "source": [
    "## Creat Image Batches and Run Inference with Images Tiling from Big Map with Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae440fb-2af9-4c51-ae6e-1e944d0f668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for images tiling from a big map\n",
    "\n",
    "import shutil\n",
    "\n",
    "os.makedirs(\"data/ThreeClassManualRemove0s/test\", exist_ok=True)\n",
    "os.makedirs(\"data/ThreeClassManualRemove0s/test/unknown\", exist_ok=True)\n",
    "source_path = \"data/DinosaurNationalMonument/20220514/224/\"\n",
    "target_path = \"data/ThreeClassManualRemove0s/test/unknown/\"\n",
    "#source_path = \"data/NMPanorama/224/\"\n",
    "#target_path = \"data/NMPanorama/224/unknown/\"\n",
    "if not os.path.exists(target_path):\n",
    "    os.makedirs(target_path)\n",
    "if os.path.exists(source_path):\n",
    "    shutil.rmtree(target_path)\n",
    "shutil.copytree(source_path, target_path)\n",
    "\n",
    "print('copy dir finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe26bc9-f48b-4b8b-affb-398bf2c4c780",
   "metadata": {},
   "source": [
    "### Run batch inference with PyTorch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416bab2a-3659-434d-b37f-b581c72c4997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"custom ImageFolder to get the filepaths along with the image and label data.\"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        paths = ((self.imgs[index][0]),)\n",
    "        return super().__getitem__(index) + paths\n",
    "\n",
    "\n",
    "def infer(model, data_path: str):\n",
    "    \"\"\"give trained `model` & `data_path` where images whose\n",
    "    labels have to be predicted are kept.\n",
    "\n",
    "    `data_path`: path to data eg. ./test/<random_class>/*.png\n",
    "    it's important to have a folder with a`random class` name as ImgFolder\n",
    "    expects it.\n",
    "\n",
    "    returns: (\n",
    "        images: images loaded from disk for inferece,\n",
    "        yhats: predicted labels\n",
    "        paths: image file-path on disk        )\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize(*imagenet_stats)]\n",
    "    )\n",
    "    data = ImageFolderWithPaths(data_path, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        data,\n",
    "        batch_size=16,\n",
    "    )\n",
    "    yhats = []\n",
    "    images = []\n",
    "    paths = []\n",
    "    \n",
    "    infer_time_sum = 0\n",
    "    start_time = time.perf_counter()\n",
    "    for (imgs, _, fpaths) in dataloader:\n",
    "        start = time.perf_counter()\n",
    "        yhat = model(imgs)\n",
    "        end = time.perf_counter()\n",
    "        time_torch = end - start\n",
    "        yhat = yhat.max(1)[1]\n",
    "        yhat = yhat.data.cpu().numpy()\n",
    "        yhats.extend(yhat)\n",
    "        paths.extend(fpaths)\n",
    "        images.extend(imgs.data.cpu())\n",
    "        infer_time_sum += time_torch\n",
    "    end_time = time.perf_counter()    \n",
    "    print(\n",
    "        f\"PyTorch model on CPU with batch inference: {infer_time_sum} total time, \"\n",
    "    )\n",
    "    print(\"Overall running time elapsed for PyTorch format batch inference: \", end_time - start_time)\n",
    "    return images, yhats, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d5640-2cbf-40ed-ab6a-2d3e8c9a4539",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_stats = [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]\n",
    "images, yhats, img_paths = infer(\n",
    "    scratch_model, data_path=\"./data/ThreeClassManualRemove0s/test\"\n",
    ")\n",
    "#infer_dataloader = DataLoader([*zip(images, yhats)], batch_size=100, shuffle=False)\n",
    "#print(\"infered images with labels\")\n",
    "#show_data(infer_dataloader, imagenet_stats, 10, figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7accbdf4-0c89-4cc8-87a1-6fcc8a794c2f",
   "metadata": {},
   "source": [
    "### Run batch inference with OpenVINO IR formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c7c8d4-ec43-4190-9f65-89dc76c44942",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"data/ThreeClassManualRemove0s/test/\"\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(*imagenet_stats)]\n",
    ")\n",
    "data = ImageFolderWithPaths(data_path, transform=transform)\n",
    "dataloader = DataLoader(\n",
    "    data,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6847f4-2340-4eeb-9b0a-4e200a186c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openvino.runtime import Core, PartialShape\n",
    "\n",
    "batch_size = 16\n",
    "model_ir = core.read_model(model=ir_path)\n",
    "ir_input_layer = model_ir.input(0)\n",
    "ir_output_layer = model_ir.output(0)\n",
    "new_shape = PartialShape([batch_size, 3, 224, 224])\n",
    "model_ir.reshape({ir_input_layer.any_name: new_shape})\n",
    "config = {\"PERFORMANCE_HINT\": \"THROUGHPUT\"}\n",
    "compiled_model_ir = core.compile_model(model_ir, \"BATCH:GPU\", config)\n",
    "\n",
    "imgs = [img for img in dataloader]\n",
    "score_batch = []\n",
    "infer_time_sum = 0\n",
    "start_time = time.time()\n",
    "for img_infer in imgs:\n",
    "    \n",
    "    # For the last batch, the number of input images may be less than previous ones\n",
    "    # Need to reshape the model for the last batch with smaller batch size\n",
    "    if img_infer[0].shape[0] != batch_size:\n",
    "        batch_size = img_infer[0].shape[0]\n",
    "        new_shape = PartialShape([batch_size, 3, 224, 224])\n",
    "        model_ir.reshape({ir_input_layer.any_name: new_shape})\n",
    "        compiled_model_ir = core.compile_model(model_ir, \"BATCH:GPU\", config)\n",
    "    \n",
    "    input_data = img_infer[0].numpy()\n",
    "    # Batch inference\n",
    "    start = time.perf_counter()\n",
    "    output = compiled_model_ir([input_data])\n",
    "    end = time.perf_counter()\n",
    "    time_torch = end - start\n",
    "    \n",
    "    # Collect the inference results for each batch\n",
    "    for index in range(batch_size):\n",
    "        res = list(output.values())[0][index]\n",
    "        score_batch.append(np.argmax(res))\n",
    "    \n",
    "    # Calculate the inference time for all batches\n",
    "    infer_time_sum += time_torch\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\n",
    "    f\"IR model on GPU for batch inference: {infer_time_sum} total time, \"\n",
    ")\n",
    "print(\"Overall running time elapsed for OpenVINO batch inference: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da75e283-fa63-4361-9f42-132e331e7c63",
   "metadata": {},
   "source": [
    "## Running inference with OpenVINO IR format under AUTO-batching feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863ac58-5605-4fe5-9827-5254d3704b4f",
   "metadata": {},
   "source": [
    "The Automatic-Batching is a new functionality in the OpenVINO™ toolkit. It performs on-the-fly automatic batching (i.e. grouping inference requests together) to improve device utilization, with no programming effort from the user. Therefore, there is no need for the users to create data in batches for inference, while auto-batching will perform batch inference behind the scene automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12250362-076e-426f-8c04-01412499f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_callback(infer_request: InferRequest, image_path: str) -> None:\n",
    "    \n",
    "    predictions = next(iter(infer_request.results.values()))\n",
    "    auto_pred = np.argmax(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470af61-92ce-42c5-8677-1982534fd751",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for x in range(xblock):\n",
    "    for y in range(yblock):\n",
    "        filename = 'assets/224/DNM_x{:02d}y{:02d}.jpg'.format(x, y)\n",
    "        filenameMap = 'assets/224Map_auto/DNM_x{:02d}y{:02d}.png'.format(x, y)\n",
    "        x_test2 = image_preprocessing(filename)\n",
    "        imgs.append(x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee080a-ed8a-4216-8249-570078f339f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensors = imgs\n",
    "\n",
    "model_ir = core.read_model(model=ir_path)\n",
    "\n",
    "# --------------------------- Step 1. Loading model to the device -----------------------------------------------------\n",
    "compiled_model_async = core.compile_model(model_ir, \"BATCH:CPU(16)\")\n",
    "\n",
    "opt_nireq = compiled_model_async.get_property('OPTIMAL_NUMBER_OF_INFER_REQUESTS')\n",
    "print('OPTIMAL_NUMBER_OF_INFER_REQUESTS', opt_nireq)\n",
    "\n",
    "config = {\"PERFORMANCE_HINT\": \"THROUGHPUT\"}\n",
    "num_ireq = [4, 256, opt_nireq][1]\n",
    "\n",
    "# --------------------------- Step 2. Create infer request queue ------------------------------------------------------\n",
    "# create async queue with optimal number of infer requests\n",
    "infer_queue = AsyncInferQueue(compiled_model_async, num_ireq)\n",
    "infer_queue.set_callback(completion_callback)\n",
    "\n",
    "# --------------------------- Step 3. Do inference --------------------------------------------------------------------\n",
    "start = time.time()\n",
    "for i, input_tensor in enumerate(input_tensors):\n",
    "    infer_queue.start_async({0: input_tensor})\n",
    "end = time.time()    \n",
    "infer_queue.wait_all()\n",
    "\n",
    "print(\"Inferencing time elapsed for OpenVINO with auto batching: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f02883-19c6-4be1-b2b7-aa456c50a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get inference results from auto-batching inference queue\n",
    "score_auto = []\n",
    "for n in range(len(imgs)):\n",
    "    results = infer_queue[n].get_output_tensor().data\n",
    "    predArgmax = np.argmax(results)\n",
    "    score_auto.append(predArgmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b782e-e21a-493b-b91d-30ea3a465fab",
   "metadata": {},
   "source": [
    "### Compare the results/accuracy between single image inference and auto-batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a493c-3936-4616-9c7c-ba510a1c80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for k in range(len(score_ir)):\n",
    "    if int(score_auto[k]) != int(score_ir[k][0]):\n",
    "        counter += 1\n",
    "        print('comparing between OpenVINO IR formats with single image inference and auto-batching')\n",
    "        print(\"Inference results are different in image\", k)\n",
    "        print(score_auto[k])\n",
    "        print(score_ir[k][0])\n",
    "print('counter is', counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41b32d-371c-43f3-afd5-ef9b7c60f95c",
   "metadata": {},
   "source": [
    "### Show inference results on images merged into a big map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9904f-af60-46a0-80b6-2e11f1a0c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_to_map(pred, filenameMap):\n",
    "    \"\"\"Color each 224 * 224 map with inference results corresponded colors.\n",
    "       Green for 'Bones highly likely', lightGreen for 'Bones possible', and white for 'No Bones'.\n",
    "    \"\"\"\n",
    "    if pred == '2':\n",
    "        Image.alpha_composite(img.convert(\"RGBA\"), green).save(filenameMap)\n",
    "    elif pred == '1': \n",
    "        Image.alpha_composite(img.convert(\"RGBA\"), lightGreen).save(filenameMap)\n",
    "    else:\n",
    "        Image.alpha_composite(img.convert(\"RGBA\"), black).save(filenameMap)\n",
    "        \n",
    "def save_merge_map(merge_path):\n",
    "    \"\"\" Merge the 224 * 224 map into a large map for the final bone finding results.\n",
    "    \"\"\"\n",
    "    map_save = merge_path\n",
    "    xblock = 17\n",
    "    yblock = 15\n",
    "    dst = Image.new('RGB', ((xblock - 1)*224, (yblock - 1)*224))\n",
    "    for x in range(xblock):\n",
    "        for y in range(yblock):\n",
    "            path = 'assets/224Map_auto/DNM_x{:02d}y{:02d}.png'.format(x,y)\n",
    "            img = Image.open(path)\n",
    "            dst.paste(img, (x*224, y*224))\n",
    "            img.close()\n",
    "    dst.save(map_save)\n",
    "    print(map_save)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f5985-17a6-44e6-986c-ce31effc01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"assets/224Map_auto\", exist_ok=True)\n",
    "xblock = 17\n",
    "yblock = 15\n",
    "for x in range(xblock):\n",
    "    for y in range(yblock):\n",
    "        filename = 'assets/224/DNM_x{:02d}y{:02d}.jpg'.format(x, y)\n",
    "        filenameMap = 'assets/224Map_auto/DNM_x{:02d}y{:02d}.png'.format(x, y)\n",
    "        img = Image.open(filename)\n",
    "        result_to_map(str(score_auto[x*yblock+y]), filenameMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc190bb-a619-4727-9cee-17e75deab7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_path = 'assets/DNM_ThreeClassBalanced_openvino_auto.jpg'\n",
    "save_merge_map(merge_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6b0f5d-dcb8-45d8-a34e-0ece33c1b5db",
   "metadata": {},
   "source": [
    "# Run Inference with New Image Tiling Method and New Result Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa653a-b9ff-4108-9627-f4a0d1ec9264",
   "metadata": {},
   "source": [
    "## Inference Given Large Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f51cb7-fae3-4fe4-8624-cfc7e5d5f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "from os.path import exists\n",
    "import os\n",
    "path = \"data/DinosaurNationalMonument/Dinosaur National Monument Panorama.png\"\n",
    "#path = \"data/Explore/Pombal01.PNG\"\n",
    "\n",
    "img = Image.open(path).convert('RGB')\n",
    "img.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ab9b4-82c4-4f8a-ab12-2ac16f8b09b6",
   "metadata": {},
   "source": [
    "# Define a Simple Eval Function: single instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02fd688-f2c2-47e1-bd91-11ea3bfbaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(image):\n",
    "    my_transforms = transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                            [0.485, 0.456, 0.406],\n",
    "                                            [0.229, 0.224, 0.225])])\n",
    "    return my_transforms(image).unsqueeze(0)\n",
    "\n",
    "def eval_simple_torch(working_slice, scratch_model):\n",
    "    x = transform_image(working_slice)\n",
    "    output = scratch_model(x)\n",
    "    return output.detach().numpy().argmax()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ea786-b247-4569-b7d0-5c983915d7f4",
   "metadata": {},
   "source": [
    "### Score large map with PyTorch format in single image inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fcfec9-3997-4516-986d-326174bb3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from os.path import exists\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "#path = \"data/DinosaurNationalMonument/Dinosaur National Monument Panorama.png\"\n",
    "#path = \"data/Test/GreenColorado.jpg\"\n",
    "#path = \"data/Test/NMMystery1.jpg\"\n",
    "#path = \"data/Test/MoabMillCreek.jpg\"\n",
    "img = Image.open(path).convert('RGB')\n",
    "\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "countBuf_torch = np.ones( (img.size[0], img.size[1]) )\n",
    "sumBuf_torch =   np.zeros( (img.size[0], img.size[1]) )\n",
    "\n",
    "start = time.time()\n",
    "step = 16  # choose a factor of 224: 1, 2, 4, 7, 8, 14, 16, 28, 32, 56, 112, and 224. Small is smooth map, large is fast\n",
    "counts = {0:0, 1:0, 2:0}\n",
    "scale = {0:0, 1:1, 2:1}\n",
    "for x in tqdm(range(0, img.size[0]-224, step)):\n",
    "    for y in range(0, img.size[1]-224, step):  \n",
    "        bbox = (x, y, x + 224, y + 224)\n",
    "        working_slice = img.crop(bbox)\n",
    "        countBuf_torch[bbox[0]:bbox[2], bbox[1]:bbox[3]] += 1\n",
    "        sumBuf_torch[bbox[0]:bbox[2], bbox[1]:bbox[3]] += scale[eval_simple_torch(working_slice, scratch_model)]\n",
    "print(f\"step size = {step}, Elapsed: {(time.time() - start):6.1f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60acff51-8ddc-4a39-b3da-8da8c5fbbfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064419a-30b6-4eb7-8ceb-b054b654a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set opacity to 159: 0 is transparent, 255 is opaque\n",
    "meanBuf = sumBuf_torch/countBuf_torch\n",
    "mat = np.uint8(meanBuf.T*159/meanBuf.max()) # scale the opacity: 0 transpartent, 255 solid\n",
    "idx = mat < 0\n",
    "mat[idx] = 0\n",
    "output = Image.fromarray(mat)\n",
    "output.save('results/bobTile_torch.png')\n",
    "np.save('results/meanBuf_torch.npy', mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c04e62d-2507-40dc-a61b-9a46869f88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open(path)\n",
    "alpha = np.load('results/meanBuf_torch.npy')\n",
    "\n",
    "imgcv = Image.open(path.replace('.jpg','.PNG'))\n",
    "heatmap_img = np.copy(imgcv)\n",
    "alpha1D = alpha/alpha.max()*255\n",
    "\n",
    "heatmap_img[:,:,0] = alpha1D\n",
    "heatmap_img[:,:,1] = 255 - alpha1D\n",
    "heatmap_img[:,:,2] = 255 - alpha1D\n",
    "\n",
    "opacity = .5\n",
    "imgHeat = Image.blend(Image.fromarray(heatmap_img), imgcv, alpha=0.5)\n",
    "imgHeat.save('results/HeatMap_torch.png')\n",
    "print(\"Color Legend:\\n- Red: Higher Probability\\n- Blue: -Lower Probability\")\n",
    "imgHeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7058e7c-ddbe-42ac-b2d2-a2e609c559ea",
   "metadata": {},
   "source": [
    "### Score large map with OpenVINO IR format in single image inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df882f1-88f6-45fb-b567-b0ad3d29c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_simple_ir(working_slice, compiled_model_ir, output_layer_ir):\n",
    "    x = transform_image(working_slice)\n",
    "    output_ir = compiled_model_ir([x])[output_layer_ir]\n",
    "    return np.argmax(output_ir)\n",
    "\n",
    "model_ir = core.read_model(model=ir_path)\n",
    "compiled_model_ir = core.compile_model(model_ir, \"CPU\")\n",
    "output_layer_ir = compiled_model_ir.output(0)\n",
    "eval_simple_ir(working_slice, compiled_model_ir, output_layer_ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e0d3f5-dad2-4417-ba4d-91b3f4f7ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "countBuf_ir = np.ones( (img.size[0], img.size[1]) )\n",
    "sumBuf_ir =   np.zeros( (img.size[0], img.size[1]) )\n",
    "\n",
    "start = time.time()\n",
    "step = 16  # choose a factor of 224: 1, 2, 4, 7, 8, 14, 16, 28, 32, 56, 112, and 224. Small is smooth map, large is fast\n",
    "counts = {0:0, 1:0, 2:0}\n",
    "scale = {0:0, 1:1, 2:1}\n",
    "for x in tqdm(range(0, img.size[0]-224, step)):\n",
    "    for y in range(0, img.size[1]-224, step):  \n",
    "        bbox = (x, y, x + 224, y + 224)\n",
    "        working_slice = img.crop(bbox)\n",
    "        countBuf_ir[bbox[0]:bbox[2], bbox[1]:bbox[3]] += 1\n",
    "        sumBuf_ir[bbox[0]:bbox[2], bbox[1]:bbox[3]] += scale[eval_simple_ir(working_slice, compiled_model_ir, output_layer_ir)]\n",
    "print(f\"step size = {step}, Elapsed: {(time.time() - start):6.1f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55daa95-5c6d-465a-ad72-47998917a6be",
   "metadata": {},
   "source": [
    "### Compute the mean of scores for sliding tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c25a750-b6e3-46a4-be6d-a1d3b596133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set opacity to 159: 0 is transparent, 255 is opaque\n",
    "meanBuf_ir = sumBuf_ir/countBuf_ir\n",
    "mat = np.uint8(meanBuf_ir.T*159/meanBuf_ir.max()) # scale the opacity: 0 transpartent, 255 solid\n",
    "idx = mat < 0\n",
    "mat[idx] = 0\n",
    "output = Image.fromarray(mat)\n",
    "output.save('results/bobTile.png')\n",
    "np.save('results/meanBuf.npy', mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b49d6-ba6b-47f1-be8d-15f072ac970e",
   "metadata": {},
   "source": [
    "### Heatmap Approach\n",
    "\n",
    "Color Legend:\n",
    "- Bright Red: Bones more likely\n",
    "- Bright Blue: Bone not likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdadb2ff-5008-4b3f-8f59-31a50ac6a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open(path)\n",
    "alpha = np.load('results/meanBuf.npy')\n",
    "\n",
    "imgcv = Image.open(path.replace('.jpg','.PNG'))\n",
    "heatmap_img = np.copy(imgcv)\n",
    "alpha1D = alpha/alpha.max()*255\n",
    "\n",
    "heatmap_img[:,:,0] = alpha1D\n",
    "heatmap_img[:,:,1] = 255 - alpha1D\n",
    "heatmap_img[:,:,2] = 255 - alpha1D\n",
    "\n",
    "opacity = .5\n",
    "imgHeat = Image.blend(Image.fromarray(heatmap_img), imgcv, alpha=0.5)\n",
    "imgHeat.save('results/HeatMap.png')\n",
    "print(\"Color Legend:\\n- Red: Higher Probability\\n- Blue: -Lower Probability\")\n",
    "imgHeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc774d-e787-48e1-b1c4-a43dd68ab994",
   "metadata": {},
   "source": [
    "### Score large map with OpenVINO IR format and AUTO-batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ab49e-656b-4a23-9657-4e5297be544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path).convert('RGB')\n",
    "\n",
    "imgs = []\n",
    "countBuf_auto = np.ones( (img.size[0], img.size[1]) )\n",
    "sumBuf_auto =   np.zeros( (img.size[0], img.size[1]) )\n",
    "\n",
    "start = time.time()\n",
    "step = 16  # choose a factor of 224: 1, 2, 4, 7, 8, 14, 16, 28, 32, 56, 112, and 224. Small is smooth map, large is fast\n",
    "counts = {0:0, 1:0, 2:0}\n",
    "scale = {0:0, 1:1, 2:1}\n",
    "for x in tqdm(range(0, img.size[0]-224, step)):\n",
    "    for y in range(0, img.size[1]-224, step):  \n",
    "        bbox = (x, y, x + 224, y + 224)\n",
    "        working_slice = img.crop(bbox)\n",
    "        imgs.append(transform_image(working_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c537f-6171-48bc-b399-8ffa9d20ac48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47930a36-322e-487d-94a0-708717e2cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ir = core.read_model(model=ir_path)\n",
    "score_auto_new = []\n",
    "\n",
    "compiled_model_async = core.compile_model(model_ir, \"BATCH:CPU(16)\")\n",
    "\n",
    "opt_nireq = compiled_model_async.get_property('OPTIMAL_NUMBER_OF_INFER_REQUESTS')\n",
    "\n",
    "config = {\"PERFORMANCE_HINT\": \"THROUGHPUT\"}\n",
    "num_ireq = [4, 256, opt_nireq][1]\n",
    "\n",
    "# bn is the number of iterations of auto-batching inference\n",
    "# since the number of input images is too large (>50,000), need to split them according to \"OPTIMAL_NUMBER_OF_INFER_REQUESTS\"\n",
    "bn = len(range(0, len(imgs), num_ireq))\n",
    "print(\"TOTAL NUMBER OF ITERATIONS FOR AUTO BATCHING IS \", bn)\n",
    "\n",
    "# --------------------- Set up the inference queue for auto-batching and do inference ------------------------------------------\n",
    "start = time.time()\n",
    "infer_queue = AsyncInferQueue(compiled_model_async, num_ireq)\n",
    "for batch in range(0, bn): \n",
    "\n",
    "    infer_queue.set_callback(completion_callback)\n",
    "    \n",
    "    if (num_ireq+batch*num_ireq) < len(imgs):\n",
    "        input_tensors = imgs[(0+batch*num_ireq):(num_ireq+batch*num_ireq)]\n",
    "    else:\n",
    "        input_tensors = imgs[(0+batch*num_ireq):]\n",
    "        \n",
    "    for i, input_tensor in enumerate(input_tensors):\n",
    "        infer_queue.start_async({0: input_tensor})\n",
    "    \n",
    "    infer_queue.wait_all()\n",
    "    \n",
    "    for n in range(len(input_tensors)):\n",
    "        result = infer_queue[n].get_output_tensor().data\n",
    "        pred = np.argmax(result)\n",
    "        score_auto_new.append(pred)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Inferencing time elapsed for OpenVINO auto batching: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb7ef7-4d8e-4bb8-93a8-8a6b5b48ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing for inference results from iterations of auto-batching\n",
    "\n",
    "xblock = len(range(0, img.size[0]-224, step))\n",
    "yblock = len(range(0, img.size[1]-224, step))\n",
    "xindex = 0\n",
    "yindex = 0\n",
    "countBuf_auto = np.ones( (img.size[0], img.size[1]) )\n",
    "sumBuf_auto =   np.zeros( (img.size[0], img.size[1]) )\n",
    "counts = {0:0, 1:0, 2:0}\n",
    "scale = {0:0, 1:1, 2:1}\n",
    "\n",
    "for x in tqdm(range(0, img.size[0]-224, step)):\n",
    "    for y in range(0, img.size[1]-224, step):\n",
    "        bbox = (x, y, x + 224, y + 224)\n",
    "        pred = score_auto_new[xindex*yblock+yindex]\n",
    "        countBuf_auto[bbox[0]:bbox[2], bbox[1]:bbox[3]] += 1\n",
    "        sumBuf_auto[bbox[0]:bbox[2], bbox[1]:bbox[3]] += scale[pred]\n",
    "        yindex += 1\n",
    "    xindex += 1\n",
    "    yindex = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164de838-ad57-47e8-b7da-d840cae66f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.size[0]-224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a61fa-f81f-4bab-9ce2-2806ff57fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set opacity to 159: 0 is transparent, 255 is opaque\n",
    "meanBuf_auto = sumBuf_auto/countBuf_auto\n",
    "mat = np.uint8(meanBuf_auto.T*159/meanBuf_auto.max()) # scale the opacity: 0 transpartent, 255 solid\n",
    "idx = mat < 0\n",
    "mat[idx] = 0\n",
    "output = Image.fromarray(mat)\n",
    "output.save('results/bobTile_auto.png')\n",
    "np.save('results/meanBuf_auto.npy', mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b188e379-4b5e-4777-a7c9-0c19b439bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path)\n",
    "alpha = np.load('results/meanBuf_auto.npy')\n",
    "\n",
    "imgcv = Image.open(path.replace('.jpg','.PNG'))\n",
    "heatmap_img = np.copy(imgcv)\n",
    "alpha1D = alpha/alpha.max()*255\n",
    "\n",
    "heatmap_img[:,:,0] = alpha1D\n",
    "heatmap_img[:,:,1] = 255 - alpha1D\n",
    "heatmap_img[:,:,2] = 255 - alpha1D\n",
    "\n",
    "opacity = .5\n",
    "imgHeat = Image.blend(Image.fromarray(heatmap_img), imgcv, alpha=0.5)\n",
    "imgHeat.save('results/HeatMap_auto.png')\n",
    "print(\"Color Legend:\\n- Red: Higher Probability\\n- Blue: -Lower Probability\")\n",
    "imgHeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dfaeb3-7d74-411c-9ac1-63967f385b19",
   "metadata": {},
   "source": [
    "## To learn more about OpenVINO. Try the demo link below.\n",
    "* https://github.com/openvinotoolkit/openvino_notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a835ce6-61a9-41f1-9dc2-595a57514f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "dino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
