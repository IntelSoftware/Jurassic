{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2d53e-30c5-49c6-9c8b-173a6ae048de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import psutil\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6e456-a8b0-45c5-9991-ff74ea0a5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resnet18_3Class_RN_epoch_10\"\n",
    "model_name = \"bc_resnet18_simple_NOIPEX_6Epochs_gold\"\n",
    "model_read = torch.jit.load(f\"models/{model_name}.pt\")\n",
    "model_read = torch.jit.freeze(model_read)\n",
    "model_read.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad97bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_read = ipex.optimize(model_read, dtype=torch.float32)\n",
    "#model_read = model_read.to(memory_format=torch.channels_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd44ef35-ffa1-4eb4-a6b7-e7bde0cd1380",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_stats = [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"custom ImageFolder to get the filepaths along with the image and label data.\"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        paths = ((self.imgs[index][0]),)\n",
    "        return super().__getitem__(index) + paths\n",
    "\n",
    "\n",
    "def infer(model, data_path: str):\n",
    "    \"\"\"give trained `model` & `data_path` where images whose\n",
    "    labels have to be predicted are kept.\n",
    "\n",
    "    `data_path`: path to data eg. ./test/<random_class>/*.png\n",
    "    it's important to have a folder with a`random class` name as ImgFolder\n",
    "    expects it.\n",
    "\n",
    "    returns: (\n",
    "        images: images loaded from disk for inferece,\n",
    "        yhats: predicted labels\n",
    "        paths: image file-path on disk        )\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize(*imagenet_stats)]\n",
    "    )\n",
    "    data = ImageFolderWithPaths(data_path, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        data,\n",
    "        batch_size=4,\n",
    "    )\n",
    "    yhats = []\n",
    "    images = []\n",
    "    paths = []\n",
    "    for (imgs, _, fpaths) in dataloader:\n",
    "        yhat = model(imgs)\n",
    "        yhat = yhat.max(1)[1]\n",
    "        yhat = yhat.data.cpu().numpy()\n",
    "        yhats.extend(yhat)\n",
    "        paths.extend(fpaths)\n",
    "        images.extend(imgs.data.cpu())\n",
    "    return images, yhats, paths\n",
    "\n",
    "def show_data(dataloader, imagenet_stats=imagenet_stats, num_data=4, figsize=(10, 6)):\n",
    "    \"\"\"show `num_data` of images and labels from dataloader.\"\"\"\n",
    "    batch = next(iter(dataloader))  # batch of with images, batch of labels\n",
    "    imgs, labels = (\n",
    "        batch[0][:num_data],\n",
    "        batch[1][:num_data].tolist(),\n",
    "    )  # get num_data of images, labels\n",
    "    plt.style.use(\"dark_background\")\n",
    "    _, axes = plt.subplots(1, num_data, figsize=figsize)\n",
    "    for n in range(num_data):\n",
    "        axes[n].set_title(labels[n])\n",
    "        imgs[n] = _denormalize(imgs[n], imagenet_stats)\n",
    "        axes[n].imshow(torch.clamp(imgs[n], 0, 1).permute(1, 2, 0))\n",
    "\n",
    "def _denormalize(images, imagenet_stats):\n",
    "    \"\"\"de normalize dataset using imagenet std and mean to show images\"\"\"\n",
    "    mean = torch.tensor(imagenet_stats[0]).reshape(1, 3, 1, 1)\n",
    "    std = torch.tensor(imagenet_stats[1]).reshape(1, 3, 1, 1)\n",
    "    return images * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96604681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb9840-d28a-4f62-a16e-62513576bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/ThreeClassManualRemove0s/test\n",
    "!mkdir  -p data/ThreeClassManualRemove0s/test/unknown\n",
    "#!cp data/DinosaurNationalMonument/20220514/224/*.jpg data/ThreeClassManualRemove0s/test/unknown/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad6b8e2-90a0-416e-ac73-f0c755099072",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_stats = [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]\n",
    "images, yhats, img_paths = infer(\n",
    "    model_read, data_path=\"./data/ThreeClassManualRemove0s/test/\"\n",
    ")\n",
    "infer_dataloader = DataLoader([*zip(images, yhats)], batch_size=100, shuffle=False)\n",
    "print(\"infered images with labels\")\n",
    "show_data(infer_dataloader, imagenet_stats, 10, figsize=(20, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5adea7a-6963-4c42-b43f-2ce55ea5a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "from os.path import exists\n",
    "import os\n",
    "path = \"data/DinosaurNationalMonument/Dinosaur National Monument Panorama.png\"\n",
    "img = Image.open(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd4d75-ef40-4c55-96bd-91d3f9e1206a",
   "metadata": {},
   "source": [
    "# Get sample near DNM Excavation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c91fa2-7928-4bf5-9922-1a0b3090ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = 4*224+112\n",
    "low = 4*224+112\n",
    "bbox = (left, low, left + 224, low + 224)\n",
    "working_slice = img.crop(bbox)\n",
    "working_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf6b2a3-ccdb-43a9-8c40-e26457ffc40a",
   "metadata": {},
   "source": [
    "# Get sample at DNM near top of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae8824-7c17-450b-85ce-5151fd9a3294",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = 4*224+112\n",
    "low = 0*224+112\n",
    "bbox = (left, low, left + 224, low + 224)\n",
    "working_slice = img.crop(bbox)\n",
    "working_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3d10a-e7af-48ff-a4b3-3a1c0c4995d6",
   "metadata": {},
   "source": [
    "# Simple eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a40c23-7986-49ba-ac8c-e1ffc6defd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_read = model_read.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836302a0-e03d-4ee0-9953-4259dc723251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as tvision\n",
    "\n",
    "left = 4*224\n",
    "low = 4*224\n",
    "bbox = (left, low, left + 224, low + 224)\n",
    "working_slice = img.crop(bbox)\n",
    "working_slice\n",
    "\n",
    "def eval_simple(working_slice):\n",
    "    x = tvision.to_tensor(working_slice)\n",
    "    x.unsqueeze_(0)\n",
    "    x = x.to(memory_format=torch.channels_last)\n",
    "    output = model_read(x)\n",
    "    return output#.detach().numpy().argmax()\n",
    "\n",
    "def eval_batch(working_slices):\n",
    "    x = torch.from_numpy(working_slices)\n",
    "    #x.unsqueeze_(0)\n",
    "    x = x.to(memory_format=torch.channels_last)\n",
    "    outputs = model_read(x)\n",
    "    return outputs#.detach().numpy().argmax()\n",
    "\n",
    "\n",
    "eval_simple(working_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53009f4-b70a-4752-811c-df810354eb22",
   "metadata": {},
   "source": [
    "# Score large map of DNM\n",
    "\n",
    "<img src=\"data/DinosaurNationalMonument/Dinosaur National Monument Panorama.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646b75d-dc7b-4ace-9356-5b1446fc516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "step = 28\n",
    "# counts = {0:0, 1:0, 2:0}\n",
    "# scale = {0:-10, 1:0, 2:10}\n",
    "\n",
    "\n",
    "def img_crops(img):\n",
    "    working_slices = []\n",
    "    for x in tqdm(range(0, img.size[0]-224, step)):\n",
    "        for y in range(0, img.size[1]-224, step):  \n",
    "            bbox = (x, y, x + 224, y + 224)\n",
    "            working_slices.append(np.asarray(img.crop(bbox), dtype=\"float32\"))\n",
    "    return working_slices\n",
    "\n",
    "\n",
    "working_slices = img_crops(img)\n",
    "\n",
    "class SlicesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_list):\n",
    "        super().__init__()\n",
    "        self.img_list = img_list\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), \n",
    "             transforms.Normalize(*imagenet_stats)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.img_list[idx]\n",
    "        return self.transform(img)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359851e-bb67-42e3-8cf5-384aceda3cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152dc50f-bb7a-4b4c-881b-8fa71de3766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "torch.set_num_threads(8)\n",
    "torch.set_num_interop_threads(2)\n",
    "print(torch.get_num_interop_threads())\n",
    "print(torch.get_num_threads())\n",
    "os.environ[\"KMP_AFFINITY\"] = \"granularity=fine,compact,1,0\"\n",
    "st_time = time.time()\n",
    "yhats = []\n",
    "for patch in DataLoader(SlicesDataset(working_slices), batch_size=8):\n",
    "    yhat = model_read(patch)\n",
    "    yhat = torch.log_softmax(yhat, dim=1)  # softmax of logit output\n",
    "    yhat = yhat.max(1)[1]\n",
    "    yhats.append(yhat)\n",
    "print(f\"total time: {time.time() - st_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c39a0-3db0-4866-bcb0-495fe27cf564",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_read(patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16761e-75a1-460d-949a-a1a0777b30dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import time\n",
    "\n",
    "# from PIL import Image\n",
    "# countBuf = np.ones( (img.size[0], img.size[1]) )\n",
    "# sumBuf =   np.zeros( (img.size[0], img.size[1]) )\n",
    "\n",
    "# start = time.time()\n",
    "# step = 28\n",
    "# counts = {0:0, 1:0, 2:0}\n",
    "# #scale = {0:-10, 1:0, 2:10}\n",
    "# for x in tqdm(range(0, img.size[0]-224, step)):\n",
    "#     for y in range(0, img.size[1]-224, step):  \n",
    "#         bbox = (x, y, x + 224, y + 224)\n",
    "#         working_slice = img.crop(bbox)\n",
    "#         countBuf[bbox[0]:bbox[2], bbox[1]:bbox[3]] += 1\n",
    "#         sumBuf[bbox[0]:bbox[2], bbox[1]:bbox[3]] += eval_simple(working_slice)\n",
    "#         counts[eval_simple(working_slice)] += 1\n",
    "#         #print(x, y, eval_simple(working_slice))\n",
    "# print(f\"step size = {step}, Elapsed: {(time.time() - start):6.1f} sec\")\n",
    "# counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f2aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940456f-2c00-4b1a-9f94-c74c46d2ff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "torch.set_num_threads(8)\n",
    "torch.set_num_interop_threads(1)\n",
    "print(torch.get_num_interop_threads())\n",
    "print(torch.get_num_threads())\n",
    "os.environ[\"KMP_AFFINITY\"] = \"granularity=fine,compact,1,0\"\n",
    "#%lprun -f xy xy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365d563-bb10-46c1-bd44-04ffa8aaeb88",
   "metadata": {},
   "source": [
    "# Compute the mean of scores for sliding tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29650efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae35ef3-fe56-430d-813f-878c547c5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#set transparency to 159: 0 is transparent, 255 is opaque\n",
    "meanBuf = sumBuf/countBuf\n",
    "mat = np.uint8(meanBuf.T*159/meanBuf.max())\n",
    "output = Image.fromarray(mat)\n",
    "output.save('data/bobTile.png')\n",
    "np.save('data/meanBuf.npy', mat)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43fd667-712c-43ba-b4ac-e1223631eaf7",
   "metadata": {},
   "source": [
    "# Create RGBA image with green transparency mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b7e76-b9dd-4c40-967b-d587346f2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from PIL import Image\n",
    "path = \"data/DinosaurNationalMonument/Dinosaur National Monument Panorama.png\"\n",
    "img = Image.open(path)\n",
    "green = Image.new('RGBA',meanBuf.shape,(0,255,0,255))\n",
    "alpha = np.uint8(meanBuf.T*159/meanBuf.max())\n",
    "#green.putalpha(alpha)\n",
    "mask = Image.fromarray(alpha)\n",
    "img.paste(green, (0, 0), mask)\n",
    "img.save('data/bobNewMap.png')\n",
    "img\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d86d8a-5e16-4825-b2c3-ef58d020f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.histogram(alpha, bins=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2396f5-ebc4-4a36-82c6-61fd27830d6b",
   "metadata": {},
   "source": [
    "# NumPy Select: Color threshold differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32929b10-2493-4792-abd4-aa9fa874bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "path = \"data/DinosaurNationalMonument/Dinosaur National Monument Panorama.png\"\n",
    "img = Image.open(path)\n",
    "condition = [ (alpha  < 100) ,\n",
    "              ( alpha >= 100) & (alpha < 140)]\n",
    "choice = [ 0, \n",
    "           64 ]\n",
    "default = [(128)]\n",
    "newAlpha = np.select(condition, choice, default= default )\n",
    "\n",
    "mask = Image.fromarray(newAlpha.astype('uint8'))\n",
    "img.paste(green, (0, 0), mask)\n",
    "img.save('data/bobNewMapSelect.png')\n",
    "img\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246745ef-6357-4030-82a3-ca770928a84a",
   "metadata": {},
   "source": [
    "# Heatmap Approach\n",
    "\n",
    "Color Legend:\n",
    "- Bright Red: Bones more likely\n",
    "- Bright Blue: Bone not likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f5e23-bd0f-4e32-ac3b-c0b14c115c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from PIL import ImageDraw\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "alpha = np.load('data/meanBuf.npy')\n",
    "caption = ImageDraw.Draw(img)\n",
    "imgcv = cv2.imread(path)\n",
    "heatmap_img = np.copy(imgcv)\n",
    "alpha1D = alpha/alpha.max()*255\n",
    "heatmap_img[:,:,0] = alpha1D\n",
    "heatmap_img[:,:,1] = 0\n",
    "heatmap_img[:,:,2] = 255 - alpha1D\n",
    "super_imposed_img = cv2.addWeighted(heatmap_img, 0.7, imgcv, 0.3, 0)\n",
    "imgHeat = Image.fromarray(super_imposed_img.astype('uint8'))\n",
    "imgHeat.save('data/bobHeat.png')\n",
    "\n",
    "imgHeat.show()\n",
    "print(\"Color Legend:\\n- Red: Higher Probability\\n- Blue: -Lower Probability\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c70f8-4b28-4f9c-8329-a56bb868486a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818b9c27-b620-4287-863f-889d7f32f351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
