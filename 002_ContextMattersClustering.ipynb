{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Context Matters - Clustering\n",
    "\n",
    "Lets begin by asserting that we have collected GPS location data fo various fossil samples. Many times a dinosaur bone fragment may be similiar in color, texture, and details to other fossils - So how can we gain confience that what we found was bone?\n",
    "\n",
    "### Petrified Wood\n",
    "\n",
    "![assets/PetrifiedWoodcropped1.PNG](assets/PetrifiedWoodcropped1.PNG)\n",
    "\n",
    "![assets/PetrifiedWoodcropped2.PNG](assets/PetrifiedWoodcropped2.PNG)\n",
    "\n",
    "### Shark Teeth\n",
    "\n",
    "![assets/SharkTeeth20220520_113552.JPG](assets/SharkTeeth20220520_113552.JPG)\n",
    "\n",
    "### Crocodilian Skutes\n",
    "\n",
    "![assets/CrocodilianSkutes20220520_112944.JPG](assets/CrocodilianSkutes20220520_112944.JPG)\n",
    "\n",
    "### Bones\n",
    "\n",
    "![assets/DinosaurBonesLookLike.png](assets/DinosaurBonesLookLike.png)\n",
    "\n",
    "\n",
    "Lets put all of our GPS data into a spreadsheet and perform Unsupervised learning to begin to identify context\n",
    "\n",
    "https://towardsdatascience.com/simple-example-of-2d-density-plots-in-python-83b83b934f67\n",
    "\n",
    "\n",
    "-------------\n",
    "| 1 |    2       |    3          |      4        |\n",
    "|---|------------|---------------|---------------|   \n",
    "|  2   |  4 | 6 |  8  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context matters!\n",
    "\n",
    "![assets/NGMDB_DNM.PNG](assets/NGMDB_DNM.PNG)\n",
    "\n",
    "Consult a geologic map to determine if the fossils you found are within a specified geological context - this will give you an idication of whether you found shark teeth in a marine environment or bones in a terrestrial environment. Or, it will indicate that the specimen you have is from the Triassic, Jurassic, or Cretaceous periods - these are the dinosaur periods. If geologists have already identified that your search area is a tertiary or quaternary time then what you found are NOT dinosaurs fossils.\n",
    "\n",
    "Once you find a cluster of fossils in clsoe prximity, it is more likle that the other fossils nearby are from the same or simialr specimens or ate least from the same time period\n",
    "\n",
    "In some cases, we find thousands of samples of petrified wood and no bone.\n",
    "\n",
    "Other time we find hundreds of bones fragments, but no petrified wood.\n",
    "\n",
    "Same for shark teeth. Shark teeth areas TEND to exclude bone fragments from being dinosaur bones (but this is not a hard and fast rule)\n",
    "\n",
    "In some cases we find petrified wood and crocdilian skutes mingled together and then a few hundred yards away we find dinosaur bones grouped to gether.\n",
    "\n",
    "BUT - MANY, MANY times, finding a few defintitive bone samples help us conclude the very nearby samples are likely also bone same for shark teeth - find a few definitive ones and the partial fossils are most likley also shark teeth and so on.\n",
    "\n",
    "# Goal: Identify clusters\n",
    "\n",
    "What we would like is to plot our GPS coordinates and look for patterns suggesting that fossils which are clustering together.\n",
    "\n",
    "In some cases the clusters will overlap, but in many other cases, the clusters tend to reveal that fossil of simialr nature are found together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from sklearn.datasets import make_blobs\n",
    "n_components = 4\n",
    "\n",
    "labels = ['bone', 'shark teeth', 'crocodilian', 'petrified wood']\n",
    "\n",
    "X, truth = make_blobs(n_samples=3000, centers=n_components, \n",
    "                      cluster_std = [2, 1.5, 1, 1], \n",
    "                      random_state=42)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c = truth, cmap='Set1')\n",
    "plt.title(f\"Map of a mixture of 300 fossils of {n_components} fossils types \")\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the real world, \n",
    "\n",
    "We typically cannot identify each fossil specifically\n",
    "- the fossil is too small (shape does not help)\n",
    "- the fossil is too similar to other types of fossils\n",
    "- too many species were buried in the same area\n",
    "\n",
    "## The point is: Our map typically looks like this \n",
    "\n",
    "Because we dont even know what kinds of fossils we actually have, our first step is to identify any naturally occuring clusters. Since we dont yet have any other information about the fossils - they are all colored the same - gray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=50, c = 'lightgray')\n",
    "plt.title(f\"Map of a mixture of 3000 fossils of {n_components} fossils types \")\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with KMeans \n",
    "\n",
    "For simple distributions like the one above, we might try to cluster via Kmeans.\n",
    "\n",
    "FYI: Applying the patch_sklearn() from the Intel Extensions for Scikit-learn* yirelds a speedup of 4 to 5X during the fitting of the model.\n",
    "\n",
    "## Exercise: Experiment with patching and unpatching \n",
    "\n",
    "You can patch or unpatch subsequent imports from sklearn. Just be sure to patch before you import from sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn, unpatch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(X)\n",
    "print(f\"elapsed: {time.time() - start} seconds\")\n",
    "kmeans.labels_\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], s=50, c = kmeans.labels_)\n",
    "plt.title(f\"Map of a mixture of 300 fossils of {n_components} fossils types \")\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.grid()\n",
    "\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"lower right\", title=\"Classes\")\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "plt.show()\n",
    "i =0\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "index = {}\n",
    "# create a dictionary to track indexes of each laebels class\n",
    "# identified by clustering\n",
    "for i in range(4):\n",
    "    index[i] = np.where(labels == i)\n",
    "    \n",
    "# there are 40 to 60 points in each cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context! \n",
    "\n",
    "Now identification has gotten easier\n",
    "\n",
    "We can take photographs, video, gps coordinates, measurements and head back tofor an interenet search to try to identify a even a small number of fossils from each cluster. We can choose the best specimens, or the most representative, or the ones with outer curved surfaces that may help identify the specimen\n",
    "\n",
    "Context can help identify the other unkown fossils once we get a postive identification fo a few specimens\n",
    "\n",
    "The problem with kmeans is that every time we run it - it may assign a different cluster ID to each point so sometime the blob at the lower left will be identified as cluster 2 and other runs it will be identified as cluster 3 for example\n",
    "\n",
    "So it has given us great insight and gotten us to a point where we might classify these points in a more consistnent way.\n",
    "\n",
    "### Classifier Will Color Sites Consistently\n",
    "\n",
    "#### Exercise: Experiment with patching and unpatching SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use 8 points from each class to create a classifier\n",
    "# the classifier uses 8 values of X and 8 values of y for each class \n",
    "# to fit a classifier model to the data\n",
    "# then we can use the classifier to predict \n",
    "# a bone type based on locality of the find\n",
    "\n",
    "X_train = X[index[0][0][:80]]\n",
    "X_train =  np.append(X_train, X[index[1][0][:8]], axis = 0)\n",
    "X_train =  np.append(X_train, X[index[2][0][:8]], axis = 0)\n",
    "X_train =  np.append(X_train, X[index[3][0][:8]], axis = 0)\n",
    "\n",
    "y_train = labels[index[0][0][:80]]\n",
    "y_train =  np.append(y_train, labels[index[1][0][:8]], axis = 0)\n",
    "y_train =  np.append(y_train, labels[index[2][0][:8]], axis = 0)\n",
    "y_train =  np.append(y_train, labels[index[3][0][:8]], axis = 0)\n",
    "\n",
    "patch_sklearn(\"SVC\") # patch SVC algo \n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC  # accelerated with Intel Extensions for Scikit-learn\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "start = time.time()\n",
    "clf.fit(X_train,  y_train.ravel())\n",
    "y_pred = clf.predict(X)\n",
    "print(f\"{time.time() - start} seconds\")\n",
    "plt.scatter(X[:,0], X[:,1], c = y_pred)\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with DBSCAN for more complicated data distributions\n",
    "\n",
    "We will use DBSCAN from scikit-learn to attemp to identify our clustering.\n",
    "\n",
    "BUT - DBSCAN requires a parameter called EPS - which is the minimum distance bewteen points to be considered part of the same cluster\n",
    "\n",
    "To find a good value for EPS - we will use sscikit-learn's nearest neighbor to plot the all the paris fo distances to help identify a good starting value to use for DBSCAN\n",
    "\n",
    "We plot a sorted distance curve and look for the distance that best represents a knee or sometimes the large values from the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "patch_sklearn() # patch all remaining sklearn imports\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=2).fit(X)\n",
    "distances, indices = nn.kneighbors(X)\n",
    "plt.plot(sorted(distances[:,1]))\n",
    "plt.ylabel('Distance')\n",
    "plt.xlabel('Number of finds')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knee = .4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "clustering = DBSCAN(eps = 1.4, min_samples = 5 ).fit(X)\n",
    "color = clustering.labels_\n",
    "\n",
    "labels = [str(i) for i in np.unique(clustering.labels_).tolist()]\n",
    "plt.legend(labels)\n",
    "scatter = plt.scatter(X[:,0], X[:,1], c = color)\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"lower right\", title=\"Classes\")\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: DBSCAN Identifies Outliers as class -1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel density Estimation\n",
    "\n",
    "single long vector\n",
    "\n",
    "have to reshape \n",
    "\n",
    "say that often when you get home you drop your keys and relax for dinner. Later you decide to drive to a grocery store for snacks. You've lost your keys!\n",
    "\n",
    "But you have lost your keys many times before and have kep a reocrd of exactly where you found them over the course of time\n",
    "\n",
    "It seems reasonable that the likely that finding your keys near to locations you found them previously is high. Finding them in your dining room, living room, bathroom, bedroom each have historical probabilites.\n",
    "\n",
    "On the other hand, seeing as you've never been to Nome Alaska, your likelyhood of finding your keys there is very nearly zero.\n",
    "\n",
    "Even if you have never found your keys in the bathroom before, it makes sense that the probability of finding keys in the bathroom is higher than finding your keys in Nome.  There is a sense in which the density of finds and nearby locations have higher probablities than locations very, very far away.\n",
    "\n",
    "This is similar to finding dinosaur bones, petrified wood, prehistoric shark teeth, prehistoric crocodilian remains.  The probability of finding a dinosaur bone is higher when other dinosaur bones have been found nearby\n",
    "\n",
    "This is where clustering and kernel density estimation can help to determine what a given sample of likely to be and how likely it is to find a bone in a particular location \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y\n",
    "x = X[:, 0]  # n_smaples is number of bone locations\n",
    "y = X[:, 1]\n",
    "# Define the borders\n",
    "deltaX = (max(x) - min(x))/10\n",
    "deltaY = (max(y) - min(y))/10\n",
    "xmin = min(x) - deltaX\n",
    "xmax = max(x) + deltaX\n",
    "ymin = min(y) - deltaY\n",
    "ymax = max(y) + deltaY\n",
    "print(xmin, xmax, ymin, ymax)\n",
    "# Create meshgrid\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()]) # stack into 1D for KDE calc\n",
    "values = np.vstack([x, y])\n",
    "kernel = st.gaussian_kde(values)\n",
    "f = np.reshape(kernel(positions).T, xx.shape) # reshape to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.gca()\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "cfset = ax.contourf(xx, yy, f, cmap='coolwarm')\n",
    "ax.imshow(np.rot90(f), cmap='coolwarm', extent=[xmin, xmax, ymin, ymax])\n",
    "cset = ax.contour(xx, yy, f, colors='k')\n",
    "ax.clabel(cset, inline=1, fontsize=10)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "plt.title('2D Gaussian Kernel density estimation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = f > .006\n",
    "# fig = plt.figure(figsize=(8,8))\n",
    "# ax = fig.gca()\n",
    "# ax.set_xlim(xmin, xmax)\n",
    "# ax.set_ylim(ymin, ymax)\n",
    "# ax.scatter(xx[idx], yy[idx])\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.integrate import simps\n",
    "# import numpy as np\n",
    "\n",
    "# # x = np.linspace(0, 1, 20)                # x.shape  (20,)\n",
    "# # y = np.linspace(0, 1, 30)                # y.shape  (30,)\n",
    "# # z = np.cos(x[:,None])**4 + np.sin(y)**2  #z.shape (20, 30)\n",
    "# # simps(simps(f, yy[0,:]), xx[:,0])\n",
    "\n",
    "# # -11.097585529030251 9.858180523990058 -11.545696279950441 16.81828998331647\n",
    "# simps(simps(f, yy[0,:]), xx[:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the highest peak\n",
    "\n",
    "Probability of 1 at the max location - like 100% chance Mammoth Mesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNorm = f/f.max()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.gca()\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "cfset = ax.contourf(xx, yy, fNorm, cmap='coolwarm')\n",
    "ax.imshow(np.rot90(f), cmap='coolwarm', extent=[xmin, xmax, ymin, ymax])\n",
    "cset = ax.contour(xx, yy, fNorm, colors='k')\n",
    "ax.clabel(cset, inline=1, fontsize=10)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "plt.title('2D Gaussian Kernel density estimation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
