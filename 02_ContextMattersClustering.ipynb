{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Context Matters - Clustering\n",
    "\n",
    "<img src=\"assets/GPS.jpg\" width=\"500\"/>\n",
    "\n",
    "<img src=\"assets/UncoloredBlobs.PNG\" width=\"500\"/>\n",
    "\n",
    "Lets begin by asserting that we have collected GPS location data fo various fossil samples. Many times a dinosaur bone fragment may be similiar in color, texture, and details to other fossils - So how can we gain confience that what we found was bone?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context matters!\n",
    "\n",
    "<img src=\"assets/NGMDB_DNM.jpg\" width=\"500\"/>\n",
    "\n",
    "\n",
    "Consult a geologic map to determine if the fossils you found are within a specified geological context - this will give you an indication of whether you found shark teeth in a marine environment or bones in a terrestrial environment. Also, it will indicate that the specimen you have is from the Triassic, Jurassic, or Cretaceous periods - these are the dinosaur periods. If geologists have already identified that your search area is in the tertiary or the quaternary period then what you found are NOT dinosaurs fossils.\n",
    "\n",
    "Once you find a cluster of fossils in close proximity, it is more like that the other fossils nearby are from the same or similar specimens or at least from the same time period.\n",
    "\n",
    "In some cases, we find thousands of samples of petrified wood and no bone.\n",
    "\n",
    "Other times we find hundreds of bones fragments, but no petrified wood.\n",
    "Same for shark teeth. Shark teeth areas TEND to exclude bone fragments from being dinosaur bones (but this is not a hard and fast rule)\n",
    "\n",
    "In some cases, we find petrified wood and crocodilian skutes mingled together and then a few hundred yards away we find dinosaur bones grouped together.\n",
    "\n",
    "BUT - MANY, MANY times, finding a few definitive bone samples help us conclude the very nearby samples are likely also bone same for shark teeth - find a few definitive ones and the partial fossils are most likely also shark teeth and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduce Intel AI Software Portfolio\n",
    "\n",
    "For this workshop we will be using Intel AI Analytics Toolkit (Intel Extensions for scikit-learn* and Intel Extensions for PyTorch) for preprocessing and training, and Intel OpenVINO(tm) for inference.\n",
    "\n",
    "<img src=\"assets/IntelSWPortfolio.PNG\" width=\"800\"/>\n",
    "\n",
    "We will Discuss OpenVINO(tm) in unit 9.\n",
    "\n",
    "In this unit, we introduce Intel AI Analytics Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intel AI Analytics Toolkit\n",
    "\n",
    "<img src=\"assets/Intel AI Analytics Toolkit Components.PNG\" width=\"800\"/>\n",
    "\n",
    "In this notebook we will apply Intel Extensions for scikit-learn to accelerate commonly used machine learning algorithms.\n",
    "\n",
    "See [link](https://github.com/intel/scikit-learn-intelex)  for more information including potential acceleration.\n",
    "\n",
    "Intel has provided accelerated versions of many common algorithms such as PCA, tSNE, KNN, Random Forest, SVM, Train_Test_Split, Linear Regression, ElasticNet, Ridge, Lasso, Nearest Neighbor, KMeans, DBSCAN and more\n",
    "\n",
    "<img src=\"assets/scikit-learn-acceleration-2021.2.3.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Identify clusters\n",
    "\n",
    "What we would like is to plot our GPS coordinates and look for patterns suggesting that fossils which are clustering together.\n",
    "\n",
    "In some cases, the clusters will overlap, but in many other cases, the clusters tend to reveal that fossil of similar nature are found together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the real world, \n",
    "\n",
    "We typically cannot identify each fossil specifically\n",
    "- the fossil is too small (shape does not help)\n",
    "- the fossil is too similar to other types of fossils\n",
    "- too many species were buried in the same area\n",
    "\n",
    "### The point is: Our map typically looks like this \n",
    "\n",
    "Because we don’t even know what kinds of fossils we actually have, our first step is to identify any naturally occurring clusters. Since we don’t yet have any other information about the fossils - they are all colored the same - gray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from sklearn.datasets import make_blobs\n",
    "n_components = 4\n",
    "\n",
    "labels = ['bone', 'shark teeth', 'crocodilian', 'petrified wood']\n",
    "\n",
    "X, truth = make_blobs(n_samples=3000, centers=n_components, \n",
    "                      cluster_std = [2, 1.5, 1, 1], \n",
    "                      random_state=42)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c = 'lightgray')\n",
    "plt.title(f\"Map of a mixture of 3000 fossils of {n_components} fossils types \")\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with KMeans \n",
    "\n",
    "For simple distributions like the one above, we might try to cluster via Kmeans.\n",
    "\n",
    "FYI: Applying the patch_sklearn() from the Intel Extensions for Scikit-learn* yields a speedup of 4 to 5X during the fitting of the model.\n",
    "\n",
    "## Exercise: Experiment with patching and unpatching \n",
    "\n",
    "The Intel Extensions for Scikit-learn accelerate select functions in scikit-learn such as k-means, DBSCAN, PCA, random forest, SVC, K nearest neighbor, and more.\n",
    "\n",
    "It is implemented via a patching construct\n",
    "\n",
    "After installing Intel Extensions for Scikit-learn (not necessary on Intel DevCloud), simply import and patch BEFORE you import your desired scikit-learn algorithms:\n",
    "\n",
    "\n",
    "``` python\n",
    "from sklearnex import patch_sklearn, unpatch_sklearn\n",
    "patch_sklearn()\n",
    "```\n",
    "\n",
    "These two lines can accelerate commonly used ML algorithms enormously! \n",
    "\n",
    "#### Exercise: patch subsequent imports from sklearn. Just be sure to patch before you import from sklearn\n",
    "\n",
    "Add these lines BEFORE the import of KMeans in order to apply the patch\n",
    "\n",
    "```python\n",
    "from sklearnex import patch_sklearn, unpatch_sklearn\n",
    "patch_sklearn()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert code here: replace the Broken_Code with patch above\n",
    "\n",
    "from sklearnex import patch_sklearn, unpatch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "###\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(X)\n",
    "print(f\"elapsed: {time.time() - start} seconds\")\n",
    "kmeans.labels_\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], s=50, c = kmeans.labels_)\n",
    "plt.title(f\"Map of a mixture of 300 fossils of {n_components} fossils types \")\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.grid()\n",
    "\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"lower right\", title=\"Classes\")\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "plt.show()\n",
    "i =0\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "index = {}\n",
    "# create a dictionary to track indexes of each laebels class\n",
    "# identified by clustering\n",
    "for i in range(4):\n",
    "    index[i] = np.where(labels == i)\n",
    "    \n",
    "# there are 40 to 60 points in each cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context! \n",
    "\n",
    "Now identification has gotten easier\n",
    "\n",
    "Identify one or two of these samples as dinosaur bone and chances are good the rest of the pile are bones too!\n",
    "\n",
    "<img src=\"assets/BlackBones20150319_1315IMG_6255.jpg\" width=\"500\"/>\n",
    "\n",
    "We can take photographs, video, GPS coordinates, measurements and head back for an internet search to try to identify a even a small number of fossils from each cluster. We can choose the best specimens, or the most representative, or the ones with outer curved surfaces that may help identify the specimen\n",
    "\n",
    "**Context** can help identify the other unknown fossils once we get a positive identification of a few specimens.\n",
    "\n",
    "For a fun way to think about geological context, I recommend the video \"FROM THE EARTH TO THE MOON (1998): SEASON 1, EPISODE 10 - GALILEO WAS RIGHT\". This episode recounts the Apollo 15 astronauts and backup crew go through extensive geology training in preparation for their mission. Dr. Leon (Lee) Silver articulates the importance of context in a geological setting. Context is vital for identifying dinosaur bones as well.\n",
    "\n",
    "The problem with KMeans is that every time we run it - it may assign a different cluster ID to each point so sometime the blob at the lower left will be identified as cluster 2 and other runs it will be identified as cluster 3 for example.\n",
    "\n",
    "So it has given us great insight and gotten us to a point where we might classify these points in a more consistent way.\n",
    "\n",
    "\n",
    "# Classifier Will Color Sites Consistently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s use 8 points from each class to create a classifier\n",
    "# The classifier uses 8 values of X and 8 values of y for each class \n",
    "# to fit a classifier model to the data\n",
    "# Then we can use the classifier to predict \n",
    "# a bone type based on locality of the find\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "X_train = X[index[0][0][:80]]\n",
    "X_train =  np.append(X_train, X[index[1][0][:8]], axis = 0)\n",
    "X_train =  np.append(X_train, X[index[2][0][:8]], axis = 0)\n",
    "X_train =  np.append(X_train, X[index[3][0][:8]], axis = 0)\n",
    "\n",
    "y_train = labels[index[0][0][:80]]\n",
    "y_train =  np.append(y_train, labels[index[1][0][:8]], axis = 0)\n",
    "y_train =  np.append(y_train, labels[index[2][0][:8]], axis = 0)\n",
    "y_train =  np.append(y_train, labels[index[3][0][:8]], axis = 0)\n",
    "\n",
    "patch_sklearn(\"SVC\") # patch SVC algo \n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC  # accelerated with Intel Extensions for Scikit-learn\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "start = time.time()\n",
    "clf.fit(X_train,  y_train.ravel())\n",
    "y_pred = clf.predict(X)\n",
    "print(f\"{time.time() - start} seconds\")\n",
    "plt.scatter(X[:,0], X[:,1], c = y_pred)\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Density Estimation\n",
    "\n",
    "Let’s say that often when you get home, you drop your keys and relax for dinner. Later you decide to drive to a grocery store for snacks. You've lost your keys!\n",
    "\n",
    "But you have lost your keys many times before and have kep a reocrd of exactly where you found them over the course of time\n",
    "\n",
    "It seems reasonable that the likelihood of finding your keys near to locations you found them previously is high. Finding them in your dining room, living room, bathroom, bedroom each have historical probabilites.\n",
    "\n",
    "On the other hand, seeing as you've never been to Nome Alaska, your likelyhood of finding your keys there is very nearly zero.\n",
    "\n",
    "Even if you have never found your keys in the bathroom before, it makes sense that the probability of finding keys in the bathroom is higher than finding your keys in Nome.  There is a sense in which the density of finds and nearby locations have higher probablities than locations very, very far away.\n",
    "\n",
    "This is similar to finding dinosaur bones, petrified wood, prehistoric shark teeth, prehistoric crocodilian remains.  The probability of finding a dinosaur bone is higher when other dinosaur bones have been found nearby\n",
    "\n",
    "This is where clustering and kernel density estimation can help to determine what a given sample of likely to be and how likely it is to find a bone in a particular location \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y\n",
    "x = X[:, 0]  # n_smaples is number of bone locations\n",
    "y = X[:, 1]\n",
    "# Define the borders\n",
    "deltaX = (max(x) - min(x))/10\n",
    "deltaY = (max(y) - min(y))/10\n",
    "xmin = min(x) - deltaX\n",
    "xmax = max(x) + deltaX\n",
    "ymin = min(y) - deltaY\n",
    "ymax = max(y) + deltaY\n",
    "print(xmin, xmax, ymin, ymax)\n",
    "# Create meshgrid\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()]) # stack into 1D for KDE calc\n",
    "values = np.vstack([x, y])\n",
    "kernel = st.gaussian_kde(values)\n",
    "f = np.reshape(kernel(positions).T, xx.shape) # reshape to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.gca()\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "cfset = ax.contourf(xx, yy, f, cmap='coolwarm')\n",
    "ax.imshow(np.rot90(f), cmap='coolwarm', extent=[xmin, xmax, ymin, ymax])\n",
    "cset = ax.contour(xx, yy, f, colors='k')\n",
    "ax.clabel(cset, inline=1, fontsize=10)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "plt.title('2D Gaussian Kernel density estimation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = f > .006\n",
    "# fig = plt.figure(figsize=(8,8))\n",
    "# ax = fig.gca()\n",
    "# ax.set_xlim(xmin, xmax)\n",
    "# ax.set_ylim(ymin, ymax)\n",
    "# ax.scatter(xx[idx], yy[idx])\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.integrate import simps\n",
    "# import numpy as np\n",
    "\n",
    "# # x = np.linspace(0, 1, 20)                # x.shape  (20,)\n",
    "# # y = np.linspace(0, 1, 30)                # y.shape  (30,)\n",
    "# # z = np.cos(x[:,None])**4 + np.sin(y)**2  #z.shape (20, 30)\n",
    "# # simps(simps(f, yy[0,:]), xx[:,0])\n",
    "\n",
    "# # -11.097585529030251 9.858180523990058 -11.545696279950441 16.81828998331647\n",
    "# simps(simps(f, yy[0,:]), xx[:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the highest peak\n",
    "\n",
    "Probability of 1 at the max location - like 100% chance Mammoth Mesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNorm = f/f.max()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.gca()\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "cfset = ax.contourf(xx, yy, fNorm, cmap='coolwarm')\n",
    "ax.imshow(np.rot90(f), cmap='coolwarm', extent=[xmin, xmax, ymin, ymax])\n",
    "cset = ax.contour(xx, yy, fNorm, colors='k')\n",
    "ax.clabel(cset, inline=1, fontsize=10)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "plt.title('2D Gaussian Kernel density estimation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have any issues or want to contribute, please contact our authors:\n",
    "Intel oneAPI Solution Architect\n",
    "- Chesebrough, Bob [bob.chesebrough (at) intel.com]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
