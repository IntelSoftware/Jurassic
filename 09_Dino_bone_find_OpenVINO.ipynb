{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ebc3d07-6666-4685-aa67-1604cb21451b",
   "metadata": {},
   "source": [
    "# Dino Bone Finding PyTorch Model to OpenVINO for High-Performance AI Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fc5437-8c35-44cf-b1ec-7eb3fa919cae",
   "metadata": {},
   "source": [
    "<img src=\"assets/openVINO-OptimizationOverview.PNG\" width=\"800\"/>\n",
    "\n",
    "This notebook demonstrates the steps in converting the dinosour bone finding PyTorch model to OpenVINO Intermediate Representation (IR) format.\n",
    "\n",
    "First, the PyTorch model is converted and exported to ONNX. Then, the ONNX model is either fed directly into OpenVINO Runtime, or the ONNX model is then convert and optimized in the OpenVINO Intermediate Representation (IR) formats. Both ONNX and IR models are executed on OpenVINO Inference Engine to show model inferencing on CPU, iGPU and/or VPU devices interchangeably.\n",
    "\n",
    "If you have any issues or want to contribute, please contact our authors: \n",
    "\n",
    "Intel OpenVINO Edge AI Software Evangelist\n",
    "* Zhuo Wu, PhD [zhuo.wu (at) intel.com]  \n",
    "* Raymond Lo, PhD [raymond.lo (at) intel.com] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06580b-c0c2-446c-b8f9-c505f2da566c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00493b-6b94-4e31-b0af-c19a927aaa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import collections\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Markdown, display, Image\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from openvino.runtime import Core\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcb480e-af69-4944-bd35-54d0b46bef9d",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90fef3d-93b9-4105-af36-19549aabdcc2",
   "metadata": {},
   "source": [
    "Set the name for the model. Define the path for the trained dinosaur bone finding model with PyTorch format, and the path for the converted onnx and OpenVINO IR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0294ce-cfe1-499d-986b-01391b781d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_NAME = \"models\"\n",
    "BASE_MODEL_NAME = DIRECTORY_NAME + f\"/resnet18-Gold20220530\"\n",
    "\n",
    "# Paths where PyTorch, ONNX and OpenVINO IR models will be stored\n",
    "model_path = Path(BASE_MODEL_NAME).with_suffix(\".pt\")\n",
    "onnx_path = model_path.with_suffix(\".onnx\")\n",
    "ir_path = model_path.with_suffix(\".xml\")\n",
    "\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47aa5db-4ff2-431a-bf04-f701c451ec37",
   "metadata": {},
   "source": [
    "## ONNX Model Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869aedb4-b7a9-4ab4-9b74-3f6479d84430",
   "metadata": {},
   "source": [
    "The output for this cell will show some warnings. These are most likely harmless. Conversion succeeded if the last line of the output says ONNX model exported to resnet18-Gold20220530.onnx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cbb8b5-6b6d-4d8c-9189-8dd1b6112632",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_model = models.resnet18(pretrained=True)\n",
    "num_ftrs = scratch_model.fc.in_features\n",
    "classes = 3\n",
    "scratch_model.fc = nn.Linear(num_ftrs, classes)\n",
    "scratch_model.load_state_dict(torch.load('./models/resnet18-Gold20220530.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264381fb-d7e7-4feb-95e9-886a044acdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not onnx_path.exists():\n",
    "    dummy_input = torch.randn(1, 3, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "\n",
    "    # For the Fastseg model, setting do_constant_folding to False is required\n",
    "    # for PyTorch>1.5.1\n",
    "    torch.onnx.export(\n",
    "        scratch_model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=False,\n",
    "    )\n",
    "    print(f\"ONNX model exported to {onnx_path}.\")\n",
    "else:\n",
    "    print(f\"ONNX model {onnx_path} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f751eb-994f-48ec-9119-8901004116cd",
   "metadata": {},
   "source": [
    "## Convert ONNX Model to OpenVINO IR Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09405eb9-96d5-430e-a542-b91da979288b",
   "metadata": {},
   "source": [
    "Call the OpenVINO Model Optimizer tool to convert the ONNX model to OpenVINO IR with FP16 precision. The models are saved to the current directory. We add the mean values to the model and scale the output with the standard deviation with --scale_values. With these options, it is not necessary to normalize input data before propagating it through the network.\n",
    "\n",
    "See the [Model Optimizer Developer Guide](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) for more information about Model Optimizer.\n",
    "\n",
    "Executing this command may take a while. There may be some errors or warnings in the output. Model Optimization was successful if the last lines of the output include [ SUCCESS ] Generated IR version 11 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d4b85-4955-456f-98ba-14ae55f8a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the command for Model Optimizer\n",
    "mo_command = f\"\"\"mo\n",
    "                 --input_model \"{onnx_path}\"\n",
    "                 --input_shape \"[1,3, {IMAGE_HEIGHT}, {IMAGE_WIDTH}]\"\n",
    "                 --data_type FP16\n",
    "                 --output_dir \"{model_path.parent}\"\n",
    "                 \"\"\"\n",
    "mo_command = \" \".join(mo_command.split())\n",
    "print(\"Model Optimizer command to convert the ONNX model to OpenVINO:\")\n",
    "display(Markdown(f\"`{mo_command}`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5bcbd6-0d57-4d78-8ae3-e64f84f21821",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ir_path.exists():\n",
    "    print(\"Exporting ONNX model to IR... This may take a few minutes.\")\n",
    "    mo_result = %sx $mo_command\n",
    "    print(\"\\n\".join(mo_result))\n",
    "else:\n",
    "    print(f\"IR model {ir_path} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d47cff-669b-49e2-9de3-717b3102a84e",
   "metadata": {},
   "source": [
    "## Run Inference on Single Image with ONNX and IR Format and Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d201669-831e-49b6-a49d-242de5c80d5b",
   "metadata": {},
   "source": [
    "Inference Engine can load ONNX models directly. We first load the ONNX model, do inference and show the results. After that we load the model that was converted to Intermediate Representation (IR) with Model Optimizer and do inference on that model and show the results on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874984f-3f2a-4401-be26-f2ea33b416f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((input_size,input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((input_size,input_size)),        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "ImagePath = Path(\"assets/HFNoBone029.png\")\n",
    "img = Image.open(ImagePath)    \n",
    "x_test = data_transforms['val'](img)[:3]   #3 channels in case png bobc\n",
    "x_test.unsqueeze_(0)  # Add batch dimension\n",
    "x_test2 = Variable(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81c8e0-3dc7-4f4a-8fb2-f45dc5af2eb6",
   "metadata": {},
   "source": [
    "### Run Inference on ONNX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b8c1d-9351-45d5-b337-b9e7b13ec85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network to Inference Engine\n",
    "core = Core()\n",
    "model_onnx = core.read_model(model=onnx_path)\n",
    "compiled_model_onnx = core.compile_model(model=model_onnx, device_name=\"CPU\")\n",
    "\n",
    "output_layer_onnx = compiled_model_onnx.output(0)\n",
    "\n",
    "# Run inference on the input image\n",
    "res_onnx = compiled_model_onnx([x_test2])[output_layer_onnx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36add430-2891-46b3-8f39-dd9d331840d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the inference results using the onnx model format\n",
    "print(\"inference result using onnx format for the 3 classes are \", res_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33594ffc-ebc5-4e98-b5ef-88a757778002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax of vector x.\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce61420-2cac-4e07-851a-ea20ba16a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['0', '1', '2']\n",
    "predArgmax = np.argmax(res_onnx)\n",
    "confidence = softmax(res_onnx)\n",
    "score = []\n",
    "score.append(class_names[predArgmax] )\n",
    "score.append(float(confidence[0][predArgmax]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fff4b-a041-4d5e-a303-4e0b8e61c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the final score, including the class name for the input map image, and its confidence level\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f71d1f5-1055-440c-9d0d-4149e1a9d17a",
   "metadata": {},
   "source": [
    "### Run Inference on IR Format\n",
    "\n",
    "This will run the model on the CPU using OpenVINO Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb8c02-b64d-45bb-a75f-74bb463ad4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the network in Inference Engine\n",
    "core = Core()\n",
    "model_ir = core.read_model(model=ir_path)\n",
    "compiled_model_ir = core.compile_model(model=model_ir, device_name=\"CPU\")\n",
    "\n",
    "# Get input and output layers\n",
    "output_layer_ir = compiled_model_ir.output(0)\n",
    "\n",
    "# Run inference on the input image\n",
    "res_ir = compiled_model_ir([x_test2])[output_layer_ir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a579f-467c-44ef-a87e-76e7061bfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the inference results using the IR model format\n",
    "print(\"inference result using IR format for the 3 classes are \", res_ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f370a-ff62-4b0a-892a-48d47601575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predArgmax = np.argmax(res_ir)\n",
    "confidence = softmax(res_ir)\n",
    "score = []\n",
    "score.append(class_names[predArgmax] )\n",
    "score.append(float(confidence[0][predArgmax]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3284a5-fe04-4d6a-848f-9e18de9f65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the final score, including the class name for the input map image, and its confidence level\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e184a9f-a207-4258-a6f9-8b55338ea86e",
   "metadata": {},
   "source": [
    "### PyTorch Score Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e5ef3-277d-4096-8d4b-5ca3e8ef0e64",
   "metadata": {},
   "source": [
    "Compare the inference results using the PyTorch model format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55bd45-c029-433b-885d-0e725391a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    result_torch = scratch_model(torch.as_tensor(x_test2).float())\n",
    "\n",
    "predArgmax = torch.argmax(result_torch[0]).numpy()\n",
    "confidence = F.softmax(result_torch, dim=0)\n",
    "score = []\n",
    "score.append( class_names[predArgmax] )\n",
    "score.append( float(confidence[0][predArgmax]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e319add-350c-49d3-a92a-3cf5007e551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the final score, including the class name for the input map image, and its confidence level\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c54b40-8306-4a13-a4eb-f711931d9cf4",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f291b-1df7-43ef-943c-a06d98418281",
   "metadata": {},
   "source": [
    "Measure the time it takes to perform inference on a few hundred images. This gives an indication of performance. For more accurate benchmarking, please use the OpenVINO Benchmark Tool. Note that many optimizations are possible to improve the performance. See examples:  https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/104-model-tools/104-model-tools.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be153e91-682a-488a-9a23-f336d43167ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 400\n",
    "\n",
    "input_image = x_test2\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    compiled_model_onnx([x_test2])\n",
    "end = time.perf_counter()\n",
    "time_onnx = end - start\n",
    "print(\n",
    "    f\"ONNX model in Inference Engine/CPU: {time_onnx/num_images:.5f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_onnx:.2f}\"\n",
    ")\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    compiled_model_ir([input_image])\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(\n",
    "    f\"IR model in Inference Engine/CPU: {time_ir/num_images:5f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_images):\n",
    "        scratch_model(torch.as_tensor(input_image).float())\n",
    "    end = time.perf_counter()\n",
    "    time_torch = end - start\n",
    "print(\n",
    "    f\"PyTorch model on CPU: {time_torch/num_images:.5f} seconds per image, \"\n",
    "    f\"FPS: {num_images/time_torch:.2f}\"\n",
    ")\n",
    "\n",
    "if \"GPU\" in core.available_devices:\n",
    "    num_images = 2000\n",
    "\n",
    "    compiled_model_onnx_gpu = core.compile_model(model=model_onnx, device_name=\"GPU\")\n",
    "    #warm up\n",
    "    compiled_model_onnx_gpu([input_image])\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_images):\n",
    "        compiled_model_onnx_gpu([input_image])\n",
    "    end = time.perf_counter()\n",
    "    time_onnx_gpu = end - start\n",
    "    print(\n",
    "        f\"ONNX model in Inference Engine/GPU: {time_onnx_gpu/num_images:.5f} \"\n",
    "        f\"seconds per image, FPS: {num_images/time_onnx_gpu:.2f}\"\n",
    "    )\n",
    "\n",
    "    compiled_model_ir_gpu = core.compile_model(model=model_ir, device_name=\"GPU\")\n",
    "    #warm up\n",
    "    compiled_model_ir_gpu([input_image])\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_images):\n",
    "        compiled_model_ir_gpu([input_image])\n",
    "    end = time.perf_counter()\n",
    "    time_ir_gpu = end - start\n",
    "    print(\n",
    "        f\"IR model in Inference Engine/GPU: {time_ir_gpu/num_images:.5f} \"\n",
    "        f\"seconds per image, FPS: {num_images/time_ir_gpu:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d412213-2900-4dcd-a9d6-e0d11b674b05",
   "metadata": {},
   "source": [
    "## Results of Inference on Image Folder and Map Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5918a5-1d9d-409d-9593-6653830ec493",
   "metadata": {},
   "source": [
    "Best map to score\n",
    "Green square roughly 265 m x 265 m -  about 2.5 football or soccer fields long\n",
    "The are in Green is a significantly smaller search are than the entire map\n",
    "\n",
    "Run Inference on the whole folder of 224 maps and merge the final results into a big map.\n",
    "Compare the results with inference on ONNX, IR files and Pytorch format with CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ab92f-f2b6-4861-885f-152848864000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(ImagePath):\n",
    "    \"\"\"Preprocessing for the input aerial 224*224 map image.\n",
    "    \"\"\"\n",
    "    img = Image.open(ImagePath)    \n",
    "    x_test = data_transforms['val'](img)[:3]   #3 channels in case png bobc\n",
    "    x_test.unsqueeze_(0)  # Add batch dimension\n",
    "    x_test2 = Variable(x_test)\n",
    "    return x_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aed235-853e-46f8-a317-d042a3db695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreSingleImage_ir(ImagePath, compiled_model_ir, output_layer_ir, dataset_classes):\n",
    "    \"\"\"Run inference on single image with IR format.\n",
    "    \"\"\"\n",
    "    x_test2 = image_preprocessing(ImagePath)\n",
    "    \n",
    "    start_infer = time.perf_counter()\n",
    "    # Run inference on the input image\n",
    "    res_ir = compiled_model_ir([x_test2])[output_layer_ir]\n",
    "    stop_infer = time.perf_counter()\n",
    "    time_infer = stop_infer - start_infer\n",
    "    \n",
    "    predArgmax = np.argmax(res_ir)\n",
    "    confidence = softmax(res_ir)\n",
    "    score = []\n",
    "    score.append(class_names[predArgmax] )\n",
    "    score.append(float(confidence[0][predArgmax]) )\n",
    "    return score, time_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98827cdd-e8d8-4833-aea4-ec0ceb24351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreSingleImage_torch(ImagePath, model, dataset_classes):\n",
    "    \"\"\"Run inference on single image with PyTorch format.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x_test2 = image_preprocessing(ImagePath)\n",
    "    \n",
    "    start_infer = time.perf_counter()\n",
    "    output = model(x_test2)\n",
    "    stop_infer = time.perf_counter()\n",
    "    time_infer = stop_infer - start_infer\n",
    "\n",
    "    class_names = dataset_classes\n",
    "    predArgmax = torch.argmax(output[0]).numpy()\n",
    "    confidence = F.softmax(output, dim=0)\n",
    "    score = []\n",
    "    score.append( class_names[predArgmax] )\n",
    "    score.append( float(confidence[0][predArgmax]) )\n",
    "    return score, time_infer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543895a4-c812-4fcb-9f80-3fef4408982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_to_map(pred, filenameMap):\n",
    "    \"\"\"Color each 224 * 224 map with inference results corresponded colors.\n",
    "       Green for 'Bones highly likely', lightGreen for 'Bones possible', and white for 'No Bones'.\n",
    "    \"\"\"\n",
    "    if pred[0] == '2':\n",
    "        Image.alpha_composite(img.convert(\"RGBA\"), green).save(filenameMap)\n",
    "    elif pred[0] == '1': \n",
    "        Image.alpha_composite(img.convert(\"RGBA\"), lightGreen).save(filenameMap)\n",
    "    else:\n",
    "        Image.alpha_composite(img.convert(\"RGBA\"), black).save(filenameMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb51bbe-3ca8-4961-b123-cdd73f7d0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_merge_map(merge_path):\n",
    "    \"\"\" Merge the 224 * 224 map into a large map for the final bone finding results.\n",
    "    \"\"\"\n",
    "    map_save = merge_path\n",
    "    xblock = 17\n",
    "    yblock = 15\n",
    "    dst = Image.new('RGB', ((xblock - 1)*224, (yblock - 1)*224))\n",
    "    for x in range(xblock):\n",
    "        for y in range(yblock):\n",
    "            path = 'assets/224Map/DNM_x{:02d}y{:02d}.png'.format(x,y)\n",
    "            img = Image.open(path)\n",
    "            dst.paste(img, (x*224, y*224))\n",
    "            img.close()\n",
    "    dst.save(map_save)\n",
    "    print(map_save)\n",
    "    print(\"Done!\")\n",
    "def show_im(path):\n",
    "    scale = 0.25\n",
    "    display_image = Image.open(path)\n",
    "    display(display_image.resize(( int(display_image.width * scale), int(display_image.height * scale))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a47fa-1fb3-4af8-8f1a-65c218388508",
   "metadata": {},
   "outputs": [],
   "source": [
    "green = Image.new('RGBA',(224,224),(0,255,0,60))\n",
    "white = Image.new('RGBA',(224,224),(255,255,255,1))\n",
    "lightGreen = Image.new('RGBA',(224,224),(0,255,0,20))\n",
    "black = Image.new('RGBA',(224,224),(0,0,0,1))\n",
    "\n",
    "my_classes = ['0', '1', '2']\n",
    "\n",
    "lookup = {'0': 'No Bones',\n",
    "          '1': 'Bones possible',\n",
    "          '2': 'Bones highly likely'\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a0114-3f25-4148-8ffd-9ed958a6d6e6",
   "metadata": {},
   "source": [
    "# Running inference on the input images with PyTorch \n",
    "\n",
    "We will run the OpenVINO and PyTorch results side-by-side to compare the output results. Again, the true performance gain is much more visible once we increase the input size (e.g., 10000 of image patches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec0caa-aaf0-495c-a419-edff1f9b1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_model = models.resnet18(pretrained=True)\n",
    "num_ftrs = scratch_model.fc.in_features\n",
    "classes = 3\n",
    "scratch_model.fc = nn.Linear(num_ftrs, classes)\n",
    "scratch_model.load_state_dict(torch.load('./models/resnet18-Gold20220530.pt'))\n",
    "score_torch = []\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "infer_time_sum = 0\n",
    "xblock = 17\n",
    "yblock = 15\n",
    "for x in range(xblock):\n",
    "    for y in range(yblock):\n",
    "        filename = 'assets/224/DNM_x{:02d}y{:02d}.jpg'.format(x, y)\n",
    "        filenameMap = 'assets/224Map/DNM_x{:02d}y{:02d}.png'.format(x, y)\n",
    "        img = Image.open(filename)\n",
    "        try: \n",
    "            pred, time_infer = scoreSingleImage_torch(filename, scratch_model, my_classes)\n",
    "            infer_time_sum += time_infer\n",
    "            score_torch.append(pred)\n",
    "            # print out the predicted class names for the input images\n",
    "            # print(filename.split('/')[-1], lookup[pred[0]]) \n",
    "            result_to_map(pred, filenameMap)\n",
    "        except:\n",
    "            print (\"Problem\", x, y, filename)\n",
    "\n",
    "print(\"Scoring time elapsed for PyTorch:\", round(time.perf_counter() - start_time, 4), \"s\") \n",
    "print(\"Inferencing time elapsed for PyTorch:\", round(infer_time_sum, 4), \"s\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc190bb-a619-4727-9cee-17e75deab7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_path = 'assets/DNM_ThreeClassBalanced_PyTorch.jpg'\n",
    "save_merge_map(merge_path)\n",
    "show_im(merge_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa9c0a8-c327-4b39-b4c2-de4b20a548e1",
   "metadata": {},
   "source": [
    "# Running inference on the input images with OpenVINO \n",
    "This will run the inference on OpenVINO. If GPU is available, you can switch the device_name to \"GPU\" to offload work to GPU. In this case, we defined the \"AUTO\" and the workload will automatically distributed to the iGPU.\n",
    "\n",
    "Note: A larger sample size (1000+) is needed for benchmarking the performance. The performance below is for quick reference only for validating the data is accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f843f7-e115-4085-b00b-ee6cca8d6387",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model_ir_cpu_gpu = core.compile_model(model=model_ir, device_name=\"AUTO\")\n",
    "output_layer_ir = compiled_model_ir_cpu_gpu.output(0)\n",
    "score_ir = []\n",
    "start_time = time.perf_counter()\n",
    "infer_time_sum = 0\n",
    "xblock = 17\n",
    "yblock = 15\n",
    "for x in range(xblock):\n",
    "    for y in range(yblock):\n",
    "        filename = 'assets/224/DNM_x{:02d}y{:02d}.jpg'.format(x, y)\n",
    "        filenameMap = 'assets/224Map/DNM_x{:02d}y{:02d}.png'.format(x, y)\n",
    "        img = Image.open(filename)\n",
    "        try: \n",
    "            pred, time_infer = scoreSingleImage_ir(filename, compiled_model_ir_cpu_gpu, output_layer_ir, my_classes)\n",
    "            infer_time_sum += time_infer\n",
    "            score_ir.append(pred)\n",
    "            # print out the predicted class names for the input images\n",
    "            # print(filename.split('/')[-1], lookup[pred[0]])\n",
    "            result_to_map(pred, filenameMap)\n",
    "        except:\n",
    "            print (\"Problem\", x, y, filename)\n",
    "print(\"Scoring time elapsed for OpenVINO:\", round(time.perf_counter() - start_time, 4), \"s\") \n",
    "print(\"Inferencing time elapsed for OpenVINO:\", round(infer_time_sum, 4), \"s\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a3574-3bdf-42d4-9eab-418a46ee9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_path = 'assets/DNM_ThreeClassBalanced_OpenVINO.jpg'\n",
    "save_merge_map(merge_path)\n",
    "show_im(merge_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887acb25-e141-4a92-aef7-1ceac35f5a16",
   "metadata": {},
   "source": [
    "# Compare the results/accuracy between PyTorch and OpenVINO inference results\n",
    "\n",
    "We will check if scores from both models match (i.e., validating no loss in accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e05236-f0db-4398-847f-f7ec4e8af106",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for k in range(len(score_torch)):\n",
    "    if score_torch[k][0] != score_ir[k][0]:\n",
    "        counter += 1\n",
    "        print('comparing between pytorch and ir')\n",
    "        print(k)\n",
    "        print(score_torch[k][0])\n",
    "        print(score_ir[k][0])\n",
    "print('The number of mismatch is', counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dfaeb3-7d74-411c-9ac1-63967f385b19",
   "metadata": {},
   "source": [
    "## To learn more about OpenVINO. Try the demo link below.\n",
    "* https://github.com/openvinotoolkit/openvino_notebooks\n",
    "\n",
    "To further optimize the model, you can try Post Training Optimization Tools (demo below):\n",
    "* [Post-Training Quantization of PyTorch models with NNCF](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/112-pytorch-post-training-quantization-nncf/112-pytorch-post-training-quantization-nncf.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc12432-ac5c-4de5-b41b-0172958261fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvinopytorch",
   "language": "python",
   "name": "openvinopytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
