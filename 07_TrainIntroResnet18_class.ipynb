{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab53eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%pip install torchsummary\n",
    "#%pip install seaborn\n",
    "#%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef53d2c-5c27-4bc0-80e5-e622ea6f3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchsummary import summary \n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"ipex version: {ipex.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1fd4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" # torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device used: {device}\")\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"move data to device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device=device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader:\n",
    "    def __init__(self, data_loader, device):\n",
    "        self.data_loader = data_loader\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            yield to_device(batch, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 4242):\n",
    "    print(f\"seed set to: {seed}\")\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device != \"cpu\":\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def set_env_vars():\n",
    "    num_physical_cores = psutil.cpu_count(logical=False)\n",
    "    print(f\"setting omp num threads: {num_physical_cores}\")\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_physical_cores)\n",
    "    os.environ[\"KMP_AFFINITY\"] = \"granularity=fine,compact,1,0\"\n",
    "    os.environ[\"KMP_BLOCKTIME\"] = \"1\"\n",
    "    torch.set_num_interop_threads(2)\n",
    "    torch.set_num_threads(num_physical_cores)\n",
    "\n",
    "\n",
    "seed_everything(9342)\n",
    "set_env_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2412e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test dataset transforms\n",
    "imagenet_stats = [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomApply(\n",
    "            [\n",
    "                transforms.GaussianBlur(kernel_size=(1, 3), sigma=(1, 3)),\n",
    "                # transforms.RandomVerticalFlip()\n",
    "            ],\n",
    "            p=0.2,\n",
    "        ),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(180),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*imagenet_stats),\n",
    "    ]\n",
    ")\n",
    "\n",
    "valid_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*imagenet_stats),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa503e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for Training, Testing and Validation\n",
    "num_physical_cores = psutil.cpu_count(logical=False)\n",
    "batch_size = 64\n",
    "data_dir = pathlib.Path(\"./data/ThreeClassBalanced5000/\")\n",
    "TRAIN_DIR = data_dir / \"train\"\n",
    "VALID_DIR = data_dir / \"val\"\n",
    "\n",
    "train_data = datasets.ImageFolder(TRAIN_DIR, transform=train_transform)\n",
    "valid_data = datasets.ImageFolder(VALID_DIR, transform=valid_transform)\n",
    "\n",
    "train_data_size = int(len(train_data) * 0.8)\n",
    "test_data_size = len(train_data) - train_data_size\n",
    "train_data, test_data = random_split(train_data, [train_data_size, test_data_size])\n",
    "\n",
    "print(\n",
    "    f\"Dataset size:\\n training:: {len(train_data)}\\n testing: {len(test_data)}\\n validation:: {len(valid_data)}\"\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=True, num_workers=1\n",
    ")  # math.ceil(num_physical_cores / 4)), pin_memory=True)\n",
    "test_dataloader = DataLoader(\n",
    "    valid_data, batch_size=batch_size * 2, num_workers=1\n",
    ")  # math.ceil(num_physical_cores / 4)), pin_memory=True))   # use for testing at the end, not used for training or val\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_data, batch_size=batch_size * 2, num_workers=1\n",
    ")  # math.ceil(num_physical_cores / 4)), pin_memory=True)  # no backprob so can use double batch_size for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view dataset\n",
    "def denormalize(images, imagenet_stats):\n",
    "    \"\"\"de normalize dataset using imagenet std and mean\"\"\"\n",
    "    if len(images.shape) == 3:\n",
    "        images = images.unsqueeze(0)\n",
    "    mean = torch.tensor(imagenet_stats[0]).reshape(1, 3, 1, 1)\n",
    "    std = torch.tensor(imagenet_stats[1]).reshape(1, 3, 1, 1)\n",
    "    return images * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6b487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_data(dataloader, imagenet_stats, samples=4):\n",
    "    \"\"\"pick first n samples and show them.\"\"\"\n",
    "    for imgs, labls in dataloader:\n",
    "        _, ax = plt.subplots(figsize=(8, 4))\n",
    "        imgs = denormalize(imgs[:samples], imagenet_stats)\n",
    "        print(\"labels: \", labls[:samples].tolist())\n",
    "        ax.imshow(make_grid(imgs, nrow=4).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "view_data(train_dataloader, imagenet_stats, samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89065081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data\n",
    "view_data(valid_dataloader, imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinosourBoneFinder(nn.Module):\n",
    "    def __init__(self, backbone=18, simple=False):\n",
    "        super(DinosourBoneFinder, self).__init__()\n",
    "        backbones = {\n",
    "            18: models.resnet18,\n",
    "            34: models.resnet34,\n",
    "            50: models.resnet50,\n",
    "            101: models.resnet101,\n",
    "        }\n",
    "        self.network = backbones[backbone](pretrained=True)\n",
    "        for m, p in zip(self.network.modules(), self.network.parameters()):\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                p.requires_grad = False\n",
    "        if simple:\n",
    "            self.network.fc = nn.Linear(self.network.fc.in_features, 3)\n",
    "        else:\n",
    "            self.network.fc = nn.Sequential(\n",
    "                nn.Linear(self.network.fc.in_features, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.6),\n",
    "                nn.Linear(256, 3),\n",
    "            )\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        return self.network(x_batch)\n",
    "\n",
    "    def batch_training(self, batch):\n",
    "        \"\"\"training step for each batch.\"\"\"\n",
    "        images, labels = batch\n",
    "        yhats = self(images)\n",
    "        return F.cross_entropy(yhats, labels)\n",
    "\n",
    "    def batch_validation(self, batch):\n",
    "        \"\"\"return validation loss/accuracy scores at the end of each batch.\"\"\"\n",
    "        images, labels = batch\n",
    "        yhats = self(images)\n",
    "        loss = F.cross_entropy(yhats, labels)\n",
    "        acc = DinosourBoneFinder.accuracy(yhats, labels)\n",
    "        return {\"val_loss\": loss.detach(), \"val_acc\": acc}\n",
    "\n",
    "    def epoch_validation(self, outputs):\n",
    "        \"\"\"return validation loss/accuracy scores at the end of each epoch.\"\"\"\n",
    "        batch_losses = [x[\"val_loss\"] for x in outputs]\n",
    "        batch_acc = [x[\"val_acc\"] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        epoch_acc = torch.stack(batch_acc).mean()\n",
    "        return {\"val_loss\": epoch_loss.item(), \"val_acc\": epoch_acc.item()}\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(yhats, labels):\n",
    "        yhats = torch.argmax(yhats, 1)\n",
    "        return torch.sum(yhats == labels).double() / len(yhats)\n",
    "\n",
    "    def epoch_metrics(self, epoch, result):\n",
    "        \"\"\"get scores at the end of each epoch\"\"\"\n",
    "        print(\n",
    "            \"Epoch [{}], \\ntrain_loss = {:.5f} \\nvalidn_loss: {:5f} \\nvalidn_acc: {:0.5f}\".format(\n",
    "                epoch, result[\"train_loss\"], result[\"val_loss\"], result[\"val_acc\"]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b74e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_dataloader):\n",
    "    model.eval()\n",
    "    outputs = [model.batch_validation(batch) for batch in valid_dataloader]\n",
    "    return model.epoch_validation(outputs)\n",
    "\n",
    "\n",
    "def fit(epochs, model, train_dataloader, valid_dataloader, lr=2e-3, ipx=False):\n",
    "    \"\"\"fit the model for epochs using training and validation data.\n",
    "        Return metrics history, learning rates used and time taken\n",
    "        per epoch.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()  # prevent oom errors\n",
    "    hist = []\n",
    "    lrs = []\n",
    "    time_per_epoch = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    if ipx:\n",
    "        model.train()\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "        model, optimizer = ipex.optimize(\n",
    "            model, optimizer=optimizer, dtype=torch.float32\n",
    "        )\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=lr, total_steps=epochs\n",
    "    )\n",
    "    for epoch in range(epochs):\n",
    "        st_time = time.perf_counter()\n",
    "        epoch = epoch + 1\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in tqdm.tqdm(train_dataloader):\n",
    "            loss = model.batch_training(batch)\n",
    "            train_losses.append(loss.data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "        with torch.no_grad():\n",
    "            result = evaluate(model, valid_dataloader)\n",
    "        result[\"train_loss\"] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_metrics(epoch, result)\n",
    "        hist.append(result)\n",
    "        e_time = time.perf_counter()\n",
    "        time_per_epoch.append((e_time - st_time))\n",
    "    return hist, lrs, time_per_epoch\n",
    "\n",
    "\n",
    "def save(model, m_version=\"\"):\n",
    "    \"\"\"save model checkpoint\"\"\"\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        pathlib.Path(f\"./saved_checkpoints/{m_version}_model.pt\").mkdir(parents=True, exist_ok=True),\n",
    "    )\n",
    "\n",
    "\n",
    "def load(path):\n",
    "    \"\"\"load model from checkpoint\"\"\"\n",
    "    model = DinosourBoneFinder()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a15554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model, train_dataloader, valid_dataloader, test_dataloader, ipx=False):\n",
    "    \"\"\"fine tune model for 10 epochs and return model metrics\"\"\"\n",
    "    epochs = 10\n",
    "    torch.set_num_threads(10)\n",
    "    train_dataloader = DeviceDataLoader(train_dataloader, device)\n",
    "    valid_dataloader = DeviceDataLoader(valid_dataloader, device)\n",
    "    test_dataloader = DeviceDataLoader(test_dataloader, device)\n",
    "    model = to_device(model, device)\n",
    "    hist, _, time_per_epoch = fit(\n",
    "        epochs, model, train_dataloader, valid_dataloader, ipx=ipx\n",
    "    )\n",
    "    loss, acc = evaluate(model, test_dataloader).values()\n",
    "    print(\n",
    "        \"testdata loss (not seen by model): {:.4f}, accuracy: {:.4f}\".format(loss, acc)\n",
    "    )\n",
    "    return hist, time_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d6b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history, backbone=18):\n",
    "    \"\"\"plot loss and acc curves\"\"\"\n",
    "    train_loss = [x[\"train_loss\"] for x in history]\n",
    "    val_loss = [x[\"val_loss\"] for x in history]\n",
    "    val_acc = [x[\"val_acc\"] for x in history]\n",
    "    _, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 2.5))\n",
    "    ax[0].plot(train_loss, \"-o\")\n",
    "    ax[0].plot(val_loss, \"-o\")\n",
    "    ax[0].set_xlabel(\"epoch\")\n",
    "    ax[0].set_ylabel(\"loss\")\n",
    "    ax[0].set_title(f\"Train vs Validation loss with resnet-{backbone}\")\n",
    "    ax[1].plot(val_acc, \"-o\")\n",
    "    ax[1].set_xlabel(\"epoch\")\n",
    "    ax[1].set_ylabel(\"acc\")\n",
    "    ax[1].set_title(f\"Validation acc with resnet-{backbone}\")\n",
    "    ax[0].legend([\"train\", \"validation\"])\n",
    "    for x in ax:\n",
    "        x.yaxis.grid(True)\n",
    "        x.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "       \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _create_dataframe(ipx_off, ipx_on):\n",
    "    stats = pd.DataFrame()\n",
    "    stats[\"ipx_on\"] = ipx_on\n",
    "    stats[\"ipx_off\"] = ipx_off\n",
    "    return stats\n",
    "\n",
    "\n",
    "def time_comparision(ipx_off, ipx_on):\n",
    "    \"\"\"compare time taken for each epoch with ipx on/off\"\"\"\n",
    "    try:\n",
    "        import seaborn as sns\n",
    "    except ImportError:\n",
    "        print(\"seaborn not install to check running time\")\n",
    "    stats = _create_dataframe(ipx_off, ipx_on)\n",
    "    sns.set(rc={\"figure.figsize\": (8, 8)})\n",
    "    p = sns.boxplot(data=stats, linewidth=2)\n",
    "    p.set_xlabel(\"mode\")\n",
    "    p.set_ylabel(\"time in seconds\")\n",
    "    p.set_title(\"Running time comparision for resnet fintetuning with ipx on/off\")\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70341583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models with ipx off an on, using ResNet18 as the backbone\n",
    "model_small = DinosourBoneFinder(backbone=18)\n",
    "hist_small_ipx_off, time_per_epoch_small_ipx_off = fine_tune(model_small, train_dataloader, valid_dataloader, test_dataloader, ipx=False)\n",
    "model_small = DinosourBoneFinder(backbone=18)\n",
    "hist_small_ipx_on, time_per_epoch_small_ipx_on = fine_tune(model_small, train_dataloader, valid_dataloader, test_dataloader, ipx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b3bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's summarize the model\n",
    "#summary(to_device(model_small, device), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35769fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(hist_small_ipx_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939730fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare ipx off vs on time per each epoch\n",
    "time_comparision(time_per_epoch_small_ipx_off, time_per_epoch_small_ipx_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb41083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage difference between ipex_off and ipex_on\n",
    "print(f\"percentage diff in time to run (ipx off vs on): {-(np.mean(time_per_epoch_small_ipx_off) - np.mean(time_per_epoch_small_ipx_on) / np.mean(time_per_epoch_small_ipx_off) * 100):.4} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56d85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1733b34285ed74833bcb8e1b7a940727029b0435f34e09e572d05f37d583a466"
  },
  "kernelspec": {
   "display_name": "PyTorch (AI kit)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
